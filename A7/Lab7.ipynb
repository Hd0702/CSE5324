{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab7\n",
    "By: Hayden Donofrio, et al.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/haydendonofrio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 291458 unique tokens. Distilled to 291458 top words.\n"
     ]
    }
   ],
   "source": [
    "#lets prepare the data. Reading from the train.txt file\n",
    "#make sure to concatenate the test.txt and train.txt files together so we can do our own split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = []\n",
    "y = []\n",
    "data = []\n",
    "stop_words = set(stopwords.words('english')) \n",
    "with open(\"test.ft.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.split()[0] == \"__label__2\": #positive label\n",
    "            y.append(np.array([1,0]))\n",
    "        else:\n",
    "            y.append(np.array([0,1]))\n",
    "        data.append(line[1:])\n",
    "\n",
    "NUM_TOP_WORDS = None\n",
    "MAX_ART_LEN = 1000 \n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see how evenly spread our ys are to help select a metric\n",
    "# since they are one hot encoded we can just some along each column\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive class #:  200000  Negative class #:  200000\n"
     ]
    }
   ],
   "source": [
    "print(\"positive class #: \", sum(y[:,0]), \" Negative class #: \", sum(y[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the classes are evenly split! We are not specifically targeting a positive class over a negative class therefore accuracy would not be a bad metric for this dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a very large amount of data for each class. An 80/20 split should be sufficent since we have over 100,000 items for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: large_data/glove/: No such file or directory\n",
      "Found 400000 word vectors.\n",
      "(291459, 300)\n"
     ]
    }
   ],
   "source": [
    "#now lets load the pre-trained keras embedding\n",
    "!ls \"glove/\" \n",
    "EMBED_SIZE = 300\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('glove/glove.6B.300d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         87437700  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 88,159,502\n",
      "Trainable params: 721,802\n",
      "Non-trainable params: 87,437,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(embedding_layer)\n",
    "rnn.add(LSTM(300,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn.add(Dense(2, activation='sigmoid'))\n",
    "rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "50/50 [==============================] - 379s 8s/step - loss: 0.7580 - accuracy: 0.5600 - val_loss: 0.6577 - val_accuracy: 0.6288\n",
      "Epoch 2/3\n",
      "50/50 [==============================] - 447s 9s/step - loss: 0.5974 - accuracy: 0.8000 - val_loss: 0.6548 - val_accuracy: 0.6194\n",
      "Epoch 3/3\n",
      "50/50 [==============================] - 563s 11s/step - loss: 0.5544 - accuracy: 0.8600 - val_loss: 0.6500 - val_accuracy: 0.6184\n"
     ]
    }
   ],
   "source": [
    "#change this [0:50] to a larger size, I made it smaller so we can focus on graphing history and then moving it up later\n",
    "history = rnn.fit(X_train[0:50], y_train_ohe[0:50], validation_data=(X_test[0:5000], y_test_ohe[0:5000]), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         87437700  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 300)               540900    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 87,979,202\n",
      "Trainable params: 541,502\n",
      "Non-trainable params: 87,437,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#this is the gru\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU\n",
    "\n",
    "gru = Sequential()\n",
    "gru.add(embedding_layer)\n",
    "gru.add(GRU(300,activation='tanh',recurrent_activation='sigmoid',dropout=0.2, recurrent_dropout=0.2))\n",
    "gru.add(Dense(2, activation='sigmoid'))\n",
    "gru.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(gru.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_history = rnn.fit(X_train[0:50], y_train_ohe[0:50], validation_data=(X_test[0:5000], y_test_ohe[0:5000]), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
