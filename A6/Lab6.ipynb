{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE should use percision score since we care about mostly identifying who has pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import average \n",
    "from keras.models import Input, Model\n",
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.io import imshow\n",
    "import seaborn as sns\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_images = []\n",
    "healthy_images = []\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "images = glob.glob(\"chest_xray/test/NORMAL/*\")\n",
    "for image in images:\n",
    "    im = cv2.imread(image,0)\n",
    "    im = cv2.resize(im,(200,200))\n",
    "    im = im.flatten()/255 -.5\n",
    "    healthy_images.append(im)\n",
    "images = glob.glob(\"chest_xray/test/PNEUMONIA/*\")\n",
    "for image in images:\n",
    "    im = cv2.imread(image,0)\n",
    "    im = cv2.resize(im,(200,200))\n",
    "    im = im.flatten()/255 -.5\n",
    "    pneumonia_images.append(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets put them together with y values\n",
    "healthy_images\n",
    "b = np.zeros((len(healthy_images),1))\n",
    "a = np.ones((len(pneumonia_images),1))\n",
    "healthy_images = np.hstack((healthy_images, b))\n",
    "pneumonia_images = np.hstack((pneumonia_images, a))\n",
    "images = np.vstack([healthy_images, pneumonia_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = images[:,-1]\n",
    "X = images[:,:-1]\n",
    "\n",
    "# folding below\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "img_wh = 200\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, 2)\n",
    "y_train_ohe = keras.utils.to_categorical(y_train, 2)\n",
    "X_train_img = np.expand_dims(X_train.reshape((-1,img_wh,img_wh)), axis=3)\n",
    "X_test_img = np.expand_dims(X_test.reshape((-1,img_wh,img_wh)), axis=3)\n",
    "folds = StratifiedKFold(n_splits=5,random_state=1).split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this is expansion on the dataset. This is a slower way to do it. When we use a keras generator we call .fit on a python generator\n",
    "#this will yield batches\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False, #do we want to make it 0 mean\n",
    "    featurewise_std_normalization=False, #do we take the whole batch and make it 0 mean, no\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=5, # used, Int. Degree range for random rotations. Randomly rotate images 5 degrees \n",
    "    width_shift_range=0.1, # used, Float (fraction of total width). Range for random horizontal shifts. \n",
    "    height_shift_range=0.1, # used,  Float (fraction of total height). Range for random vertical shifts.\n",
    "    shear_range=0., # Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    "    zoom_range=0.,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False, \n",
    "    rescale=None) #this generator will esentially run forever. This will manipulate our data, will give us different datasets every time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 200, 200, 100)     200       \n",
      "_________________________________________________________________\n",
      "activation_70 (Activation)   (None, 200, 200, 100)     0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 200, 200, 100)     0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 200, 200, 32)      3232      \n",
      "_________________________________________________________________\n",
      "activation_71 (Activation)   (None, 200, 200, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 200, 200, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1280000)           0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 2)                 2560002   \n",
      "_________________________________________________________________\n",
      "activation_72 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,563,434\n",
      "Trainable params: 2,563,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1000)\n",
    "#Instantiate an empty model\n",
    "#get a bigger dataset\n",
    "mlp_model = Sequential()\n",
    "mlp_model.add(Dense(100, input_shape=(200,200,1)))\n",
    "mlp_model.add(Activation('relu'))\n",
    "\n",
    "mlp_model.add(Dropout(0.25))\n",
    "mlp_model.add(Dense(32))\n",
    "mlp_model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "mlp_model.add(Dropout(0.25))\n",
    "\n",
    "# Output Layer\n",
    "mlp_model.add(Flatten())\n",
    "mlp_model.add(Dense(2))\n",
    "mlp_model.add(Activation('softmax'))\n",
    "mlp_model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new fold\n",
      "Epoch 1/10\n",
      " 3/30 [==>...........................] - ETA: 6:35 - loss: 5.3378 - recall_16: 0.5625"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-9d3a4fd3cd28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m              \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m              \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_ohe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m              \u001b[0;31m#callbacks=[EarlyStopping(monitor='val_loss', patience=2)]# no early stopping to allow for roc curve avg. across folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m              ))\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "#csv_logger = CSVLogger('alexNet_log.csv', append=True, separator=';')\n",
    "mlp_histories= []\n",
    "mlp_y_preds = []\n",
    "mlp_fprs = []\n",
    "mlp_tprs = []\n",
    "mlp_aucs = []\n",
    "mlp_thresholds = []\n",
    "#datagen.fit(X_train)\n",
    "for k, (train, test) in enumerate(folds):\n",
    "    mlp_model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=[keras.metrics.Recall()])\n",
    "    print(\"new fold\")\n",
    "    datagen.fit(X_train_img)\n",
    "    histories.append(mlp_model.fit_generator(datagen.flow(X_train_img[train], y_train_ohe[train], batch_size=32),\n",
    "             steps_per_epoch=30,\n",
    "             epochs=10, verbose=1,\n",
    "             validation_data=(X_test_img,y_test_ohe)\n",
    "             #callbacks=[EarlyStopping(monitor='val_loss', patience=2)]# no early stopping to allow for roc curve avg. across folds\n",
    "             ))\n",
    "    y_pred_keras = mlp_model.predict(X_test_img).ravel()\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test_ohe.ravel(), y_pred_keras)\n",
    "    auc_keras = auc(fpr_keras, tpr_keras)\n",
    "    mlp_aucs.append(auc_keras)\n",
    "    mlp_fprs.append(fpr_keras)\n",
    "    mlp_y_preds.append(y_pred_keras)\n",
    "    mlp_tprs.append(tpr_keras)\n",
    "    mlp_thresholds.append(thresholds_keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 187, 187, 32)      6304      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 187, 187, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 93, 93, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 87, 87, 64)        100416    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 87, 87, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 43, 43, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 41, 41, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 41, 41, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 39, 39, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 39, 39, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 37, 37, 128)       295040    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4096)              169873408 \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 2002      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 191,524,506\n",
      "Trainable params: 191,524,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(1000)\n",
    "#Instantiate an empty model\n",
    "#get a bigger dataset\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=32, input_shape=(200,200,1), kernel_size=(14,14),  padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=64, kernel_size=(7,7), strides=(1,1), padding='valid', kernel_initializer='he_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid', data_format=\"channels_last\"))\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='valid',kernel_initializer='he_uniform',  data_format=\"channels_last\"))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid', data_format=\"channels_last\"))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='valid', data_format=\"channels_last\"))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# Passing it to a Fully Connected layer\n",
    "model.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 2nd Fully Connected Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 3rd Fully Connected Layer\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object _BaseKFold.split at 0x1c4825d8b8>\n",
      "new fold\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 496s 17s/step - loss: 0.6928 - recall_8: 0.6095 - val_loss: 0.6153 - val_recall_8: 0.6560\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 513s 17s/step - loss: 0.6131 - recall_8: 0.6596 - val_loss: 0.4807 - val_recall_8: 0.7200\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 511s 17s/step - loss: 0.5116 - recall_8: 0.7514 - val_loss: 0.4125 - val_recall_8: 0.8320\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 520s 17s/step - loss: 0.4526 - recall_8: 0.7808 - val_loss: 0.3955 - val_recall_8: 0.8080\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 522s 17s/step - loss: 0.4173 - recall_8: 0.8042 - val_loss: 0.3118 - val_recall_8: 0.8640\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 516s 17s/step - loss: 0.3353 - recall_8: 0.8596 - val_loss: 0.3333 - val_recall_8: 0.8400\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 520s 17s/step - loss: 0.3595 - recall_8: 0.8413 - val_loss: 0.3284 - val_recall_8: 0.8640\n",
      "new fold\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 426s 14s/step - loss: 0.3900 - recall_9: 0.8402 - val_loss: 0.3348 - val_recall_9: 0.8800\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 397s 13s/step - loss: 0.3921 - recall_9: 0.8383 - val_loss: 0.2649 - val_recall_9: 0.8800\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 388s 13s/step - loss: 0.3675 - recall_9: 0.8564 - val_loss: 0.2708 - val_recall_9: 0.9040\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 400s 13s/step - loss: 0.3809 - recall_9: 0.8348 - val_loss: 0.3335 - val_recall_9: 0.8560\n",
      "new fold\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 403s 13s/step - loss: 0.3255 - recall_10: 0.8673 - val_loss: 0.3182 - val_recall_10: 0.8960\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 400s 13s/step - loss: 0.2970 - recall_10: 0.8782 - val_loss: 0.2836 - val_recall_10: 0.8960\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 397s 13s/step - loss: 0.3592 - recall_10: 0.8399 - val_loss: 0.3511 - val_recall_10: 0.8080\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 406s 14s/step - loss: 0.2936 - recall_10: 0.8793 - val_loss: 0.2670 - val_recall_10: 0.8960\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 401s 13s/step - loss: 0.2855 - recall_10: 0.8783 - val_loss: 0.3296 - val_recall_10: 0.8720\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 408s 14s/step - loss: 0.2983 - recall_10: 0.8944 - val_loss: 0.3236 - val_recall_10: 0.8960\n",
      "new fold\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 403s 13s/step - loss: 0.3410 - recall_11: 0.8685 - val_loss: 0.2653 - val_recall_11: 0.8640\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 402s 13s/step - loss: 0.2832 - recall_11: 0.8750 - val_loss: 0.4776 - val_recall_11: 0.7840\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 400s 13s/step - loss: 0.2799 - recall_11: 0.8987 - val_loss: 0.3208 - val_recall_11: 0.8640\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "csv_logger = CSVLogger('alexNet_log.csv', append=True, separator=';')\n",
    "histories= []\n",
    "y_preds = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "aucs = []\n",
    "thresholds = []\n",
    "#datagen.fit(X_train)\n",
    "print(folds)\n",
    "for k, (train, test) in enumerate(folds):\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=[keras.metrics.Recall()])\n",
    "    print(\"new fold\")\n",
    "    datagen.fit(X_train_img)\n",
    "    histories.append(model.fit_generator(datagen.flow(X_train_img[train], y_train_ohe[train], batch_size=32),\n",
    "             steps_per_epoch=30,\n",
    "             epochs=10, verbose=1,\n",
    "             validation_data=(X_test_img,y_test_ohe)\n",
    "             callbacks=[EarlyStopping(monitor='val_loss', patience=2)] no early stopping to allow for roc curve avg. across folds\n",
    "             ))\n",
    "    y_pred_keras = model.predict(X_test_img).ravel()\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test_ohe.ravel(), y_pred_keras)\n",
    "    auc_keras = auc(fpr_keras, tpr_keras)\n",
    "    aucs.append(auc_keras)\n",
    "    fprs.append(fpr_keras)\n",
    "    y_preds.append(y_pred_keras)\n",
    "    tprs.append(tpr_keras)\n",
    "    mlp_thresholds.append(thresholds_keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46905583, 0.53094417],\n",
       "       [0.4687977 , 0.5312023 ],\n",
       "       [0.469013  , 0.5309871 ],\n",
       "       [0.46885034, 0.53114974],\n",
       "       [0.46880445, 0.5311956 ],\n",
       "       [0.46898493, 0.5310151 ],\n",
       "       [0.46896693, 0.5310331 ],\n",
       "       [0.4689211 , 0.531079  ],\n",
       "       [0.46838337, 0.53161657],\n",
       "       [0.46904942, 0.5309506 ],\n",
       "       [0.46890095, 0.5310991 ],\n",
       "       [0.46905193, 0.53094804],\n",
       "       [0.4690436 , 0.5309563 ],\n",
       "       [0.46896902, 0.53103095],\n",
       "       [0.46904603, 0.530954  ],\n",
       "       [0.46898943, 0.53101057],\n",
       "       [0.469058  , 0.530942  ],\n",
       "       [0.469044  , 0.5309559 ],\n",
       "       [0.46895677, 0.5310432 ],\n",
       "       [0.46903703, 0.53096294],\n",
       "       [0.46903488, 0.53096515],\n",
       "       [0.46905404, 0.5309459 ],\n",
       "       [0.4689777 , 0.5310223 ],\n",
       "       [0.46904337, 0.5309566 ],\n",
       "       [0.4690248 , 0.5309752 ],\n",
       "       [0.46888784, 0.5311122 ],\n",
       "       [0.46897668, 0.5310233 ],\n",
       "       [0.46892962, 0.5310704 ],\n",
       "       [0.46893668, 0.5310633 ],\n",
       "       [0.46902594, 0.5309741 ],\n",
       "       [0.4690168 , 0.5309832 ],\n",
       "       [0.46886224, 0.53113776],\n",
       "       [0.46902838, 0.53097165],\n",
       "       [0.4690316 , 0.5309684 ],\n",
       "       [0.4690372 , 0.53096277],\n",
       "       [0.4689881 , 0.5310119 ],\n",
       "       [0.4689621 , 0.5310379 ],\n",
       "       [0.46881247, 0.53118753],\n",
       "       [0.46890447, 0.5310955 ],\n",
       "       [0.46902433, 0.5309757 ],\n",
       "       [0.469058  , 0.53094196],\n",
       "       [0.46905038, 0.5309497 ],\n",
       "       [0.46897703, 0.53102297],\n",
       "       [0.4690417 , 0.5309583 ],\n",
       "       [0.46893236, 0.5310676 ],\n",
       "       [0.468991  , 0.531009  ],\n",
       "       [0.46899155, 0.5310085 ],\n",
       "       [0.46903268, 0.53096735],\n",
       "       [0.46899387, 0.53100616],\n",
       "       [0.46900064, 0.53099936],\n",
       "       [0.46903747, 0.5309625 ],\n",
       "       [0.4689231 , 0.53107697],\n",
       "       [0.46904796, 0.53095204],\n",
       "       [0.46905985, 0.53094023],\n",
       "       [0.4687535 , 0.53124654],\n",
       "       [0.4690495 , 0.5309505 ],\n",
       "       [0.46904027, 0.5309598 ],\n",
       "       [0.4689371 , 0.5310629 ],\n",
       "       [0.46904403, 0.53095603],\n",
       "       [0.4690445 , 0.53095555],\n",
       "       [0.46903968, 0.5309603 ],\n",
       "       [0.46892107, 0.5310789 ],\n",
       "       [0.46905985, 0.53094023],\n",
       "       [0.4689639 , 0.5310361 ],\n",
       "       [0.4689782 , 0.53102183],\n",
       "       [0.46901166, 0.5309883 ],\n",
       "       [0.46905538, 0.53094465],\n",
       "       [0.4690159 , 0.53098416],\n",
       "       [0.46884003, 0.53115994],\n",
       "       [0.46903938, 0.53096056],\n",
       "       [0.46899867, 0.5310013 ],\n",
       "       [0.46896443, 0.5310356 ],\n",
       "       [0.4690199 , 0.5309801 ],\n",
       "       [0.46905985, 0.53094023],\n",
       "       [0.46892902, 0.531071  ],\n",
       "       [0.46904093, 0.530959  ],\n",
       "       [0.46873143, 0.5312686 ],\n",
       "       [0.46905115, 0.5309488 ],\n",
       "       [0.4690312 , 0.5309688 ],\n",
       "       [0.46901903, 0.530981  ],\n",
       "       [0.46904558, 0.5309544 ],\n",
       "       [0.46905684, 0.53094316],\n",
       "       [0.46902508, 0.5309749 ],\n",
       "       [0.469023  , 0.5309771 ],\n",
       "       [0.46905375, 0.5309462 ],\n",
       "       [0.46894085, 0.53105915],\n",
       "       [0.46905985, 0.53094023],\n",
       "       [0.46905103, 0.530949  ],\n",
       "       [0.46905568, 0.5309443 ],\n",
       "       [0.46902615, 0.5309738 ],\n",
       "       [0.46904156, 0.5309584 ],\n",
       "       [0.46905908, 0.53094095],\n",
       "       [0.4689809 , 0.53101903],\n",
       "       [0.4689779 , 0.5310221 ],\n",
       "       [0.4690338 , 0.5309662 ],\n",
       "       [0.46899116, 0.5310089 ],\n",
       "       [0.46904665, 0.53095335],\n",
       "       [0.46905985, 0.53094023],\n",
       "       [0.46880046, 0.5311996 ],\n",
       "       [0.46887282, 0.53112715],\n",
       "       [0.46904373, 0.53095627],\n",
       "       [0.4686224 , 0.5313776 ],\n",
       "       [0.46879226, 0.5312077 ],\n",
       "       [0.4690316 , 0.5309684 ],\n",
       "       [0.4690093 , 0.53099066],\n",
       "       [0.46905318, 0.53094685],\n",
       "       [0.46905985, 0.53094023],\n",
       "       [0.46905747, 0.5309425 ],\n",
       "       [0.4690384 , 0.53096163],\n",
       "       [0.46892676, 0.53107315],\n",
       "       [0.46903363, 0.5309664 ],\n",
       "       [0.46905968, 0.53094035],\n",
       "       [0.46898302, 0.53101695],\n",
       "       [0.46897846, 0.53102154],\n",
       "       [0.46901828, 0.5309817 ],\n",
       "       [0.46905985, 0.53094023],\n",
       "       [0.4685552 , 0.5314448 ],\n",
       "       [0.46905985, 0.53094023],\n",
       "       [0.46898192, 0.5310181 ],\n",
       "       [0.469033  , 0.530967  ],\n",
       "       [0.46904013, 0.5309599 ],\n",
       "       [0.46902874, 0.53097117],\n",
       "       [0.4690037 , 0.5309963 ],\n",
       "       [0.46901104, 0.530989  ],\n",
       "       [0.46902877, 0.5309712 ]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_keras = model.predict(X_test_img)\n",
    "y_pred_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.    0.    0.008 0.008 0.016 0.016 0.024 0.024 0.048 0.048 0.056\n",
      " 0.056 0.072 0.072 0.08  0.08  0.104 0.104 0.12  0.12  0.128 0.128 0.136\n",
      " 0.136 0.168 0.168 0.2   0.2   0.208 0.208 0.224 0.224 0.24  0.24  0.272\n",
      " 0.272 0.352 0.352 0.36  0.36  0.368 0.368 0.384 0.384 0.48  0.48  1.   ] [0.    0.    0.    0.008 0.008 0.016 0.016 0.024 0.024 0.032 0.032 0.04\n",
      " 0.04  0.048 0.048 0.056 0.056 0.072 0.072 0.08  0.08  0.088 0.088 0.112\n",
      " 0.112 0.12  0.12  0.128 0.128 0.144 0.144 0.152 0.152 0.176 0.176 0.2\n",
      " 0.2   0.208 0.208 0.24  0.24  0.256 0.256 0.272 0.272 0.352 0.352 0.376\n",
      " 0.376 0.392 0.392 0.416 0.416 0.44  0.44  0.56  0.56  0.664 0.664 1.   ]\n"
     ]
    }
   ],
   "source": [
    "print((fprs[0]), (fprs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5yMdfvA8c/VrhxKyTHlsA4ruzakRY7RiqTQU6KDDhZRSjrqqYRHHuSUcqZIVFI9JCUpP6XI0kZW2NZpnckxwq7r98fcu601a2fZ2dmZud6v17zc98x35r5u1l7z/X7v+/qKqmKMMSZ4XeLrAIwxxviWJQJjjAlylgiMMSbIWSIwxpggZ4nAGGOCnCUCY4wJcpYIjDEmyFkiMAFHRLaIyAkROSYiu0VkmohcnqlNQxH5VkSOishhEflcRCIztblCREaLyDbnsxKd/ZJ5e0bGeJclAhOo7lTVy4HawA3AS2kviEgD4GtgLnANUAn4FVgmIpWdNpcCi4EawG3AFUBD4ABQz1tBi0iotz7bmKxYIjABTVV3AwtxJYQ0w4D3VPVNVT2qqn+q6ivAcqC/0+YhoAJwl6omqOoZVd2rqv9R1QXujiUiNURkkYj8KSJ7ROTfzvPTRGRQhnbNRCQ5w/4WEXlRRNYAf4nIKyIyJ9NnvykiY5ztK0VkqojsEpEdIjJIREIu8q/KBDFLBCagiUg5oDWQ6OwXwfXN/mM3zWcDtzrbLYCvVPWYh8cpCnwDfIWrl1EVV4/CU/cBbYBiwAzgdhG5wvnsEOBeYJbTdjqQ4hzjBqAl0DUHxzLmLJYITKD6n4gcBbYDe4HXnOeL4/q53+XmPbuAtPH/Elm0ycodwG5VHaGqfzs9jRU5eP8YVd2uqidUdSuwGmjvvHYLcFxVl4tIGVyJ7WlV/UtV9wKjgE45OJYxZ7FEYAJVe1UtCjQDqvPPL/iDwBmgrJv3lAX2O9sHsmiTlfLAHxcUqcv2TPuzcPUSAO7nn95ARaAAsEtEDonIIWAiUPoijm2CnCUCE9BU9f+AacBwZ/8v4Cegg5vm9/LPcM43QCsRuczDQ20HqmTx2l9AkQz7V7sLNdP+x0AzZ2jrLv5JBNuBk0BJVS3mPK5Q1RoexmnMOSwRmGAwGrhVRNImjPsCD4vIUyJSVESuciZzGwADnDYzcP3S/UREqovIJSJSQkT+LSK3uznGfOBqEXlaRAo6n1vfeS0e15h/cRG5Gng6u4BVdR+wBHgX2Kyq653nd+G64mmEc3nrJSJSRURuvoC/F2MASwQmCDi/VN8DXnX2fwBaAf/CNQ+wFdeka2NV3eS0OYlrwvh3YBFwBPgZ1xDTOWP/qnoU10TzncBuYBPQ3Hl5Bq7LU7fg+iX+kYehz3JimJXp+YeAS4EEXENdc8jZMJYxZxFbmMYYY4Kb9QiMMSbIWSIwxpggZ4nAGGOCnCUCY4wJcn5X4KpkyZIaFhbm6zCMMcavrFq1ar+qlnL3mt8lgrCwMOLi4nwdhjHG+BUR2ZrVazY0ZIwxQc4SgTHGBDlLBMYYE+QsERhjTJCzRGCMMUHOa4lARN4Rkb0i8lsWr4uIjHEWBF8jInW8FYsxxpisebNHMA3Xot9ZaQ2EO4/uwHgvxmKMMSYLXruPQFWXikjYeZq0w7WAuALLRaSYiJR16q0bky/NWrGNufE7fB2GCTJnzqRy6tRp6lQuzWt35v4aRL6cI7iWs5fnS3aeO4eIdBeROBGJ27dvX54EZ4w7c+N3kLDriK/DMEHk0KFDrFwZx7p16/DWsgG+vLNY3Dzn9ixVdRIwCSA6OtoWUDCAb76dJ+w6QmTZK/josQZ5elwTfA4dOsTzzz/P7ClTqFq1KlOmTOHmm6O8cixfJoJkXAt+pykH7PRRLCafc/dLf8XmPwGoX6l4nsURWfYK2tV223E1JtekpqbSsGFDNmzYwAsvvED//v0pXLiw147ny0QwD+glIh8C9YHDNj9gspI2JBNZ9or05+pXKk672tdyf/0KPozMmNxz4MABihcvTkhICK+//jrly5cnOjra68f1WiIQkQ+AZkBJEUkGXgMKAKjqBGABcDuQCBwHHvVWLMZ/pfUEbEjGBDJVZebMmfTu3ZshQ4bQrVs37rrrrjw7vjevGrovm9cVeMJbxzeBIWMSsCEZE4i2b99Ojx49WLBgATfddBONGjXK8xj8rgy1CT7WEzCB6oMPPuCxxx4jNTWV0aNH06tXL0JCQvI8DksExhjjI1dddRX169dn0qRJVKpUyWdxWCIwxpg8kpKSwqhRozh16hQvv/wyt912G61atULE3dX0eccSgcl3Ml4qmvlKIWP81a+//kpsbCyrVq3i3nvvRVUREZ8nAbBEYC6At2/kynh/gE0SG3938uRJBg0axJAhQyhevDgff/wxd999d75IAGksEZgcc3dNf26y+wNMINm0aRNDhw7l/vvvZ+TIkZQoUcLXIZ3DEoG5IHYljzFZO3bsGHPnzuWBBx4gKiqK33//ncqVK/s6rCxZIjDpPB3ysXF7Y7K2aNEiunfvztatW6lTpw4RERH5OgmArVBmMvC0sqaN2xtzroMHDxIbG0vLli259NJL+b//+z8iIiJ8HZZHrEcQpNx9+7cyDsZcmNTUVBo1asTGjRt56aWX6NevH4UKFfJ1WB6zRBCk3E342jd9Y3Jm//796UXiBg8eTIUKFahTx/9W3bVEEMTs278xF0ZVmTFjBk8//TRDhgyhe/futG/f3tdhXTCbIzDGmBzYunUrrVu35uGHHyYiIoKmTZv6OqSLZonAGGM89P777xMVFcUPP/zAW2+9xffff0/16tV9HdZFs6GhIJF5ctguATUm50qVKkWjRo2YOHEiFStW9HU4ucYSQZDIPDlsE8PGZO/06dOMGDGC06dP8+qrr9KqVStatmyZr8pD5AZLBEHEJoeN8dwvv/xCbGwsv/zyC506dcpXReJymyWCAGZVPI3Jub///puBAwcybNgwSpYsySeffMK//vUvX4flVTZZHMAy3ilsQ0HGeCYxMZHhw4fz0EMPsX79+oBPAmA9Ar+U05pANhxkzPkdO3aMzz77jM6dOxMVFcWGDRt8umJYXrNEkM948ks+Y73+87FegDHZW7hwId27d2f79u1ER0cTERERVEkALBHkO57U+rd6/cZcvAMHDvDMM8/w3nvvUb16db7//nu/KRKX2ywR5EM2nGOMd6UViUtMTOTll1/mlVde8asicbnNEoExJmjs27ePEiVKEBISwtChQ6lYsSK1a9f2dVg+Z1cNGWMCnqry7rvvUq1aNSZPngxAu3btLAk4LBEYYwLali1baNWqFV26dOH666+nefPmvg4p37GhoXzAbvwyxjtmzJhBz549ERHGjRvHY489xiWX2PffzOxvJB+wG7+M8Y4yZcrQtGlT1q1bR8+ePS0JZMF6BPmEXSlkzMU7ffo0w4YNIzU1lX79+tGyZUtatmzp67DyPUuPxpiAsHr1aurWrcsrr7zChg0bUFVfh+Q3LBEYY/zaiRMn6Nu3L/Xq1WPPnj189tlnzJw5MyCrhHqLVxOBiNwmIhtEJFFE+rp5vYKIfCciv4jIGhG53ZvxGGMCT1JSEiNHjuSRRx4hISHBr9cO9hWvJQIRCQHGAq2BSOA+EYnM1OwVYLaq3gB0AsZ5Kx5jTOA4cuQI06ZNA6BGjRps2rSJKVOmcNVVV/k2MD/lzR5BPSBRVZNU9RTwIdAuUxsF0q6VvBLY6cV48p1ZK7bRceJP6VcMGWOyt2DBAqKiooiNjWX9+vUAAbVspC9486qha4HtGfaTgfqZ2vQHvhaRJ4HLgBbuPkhEugPdASpU8M9Ca+6qimasImqXjBpzfvv376dPnz68//77REZGsmzZsqAtEpfbvJkI3M3UZJ7Gvw+YpqojRKQBMENEolT1zFlvUp0ETAKIjo72y0sB3FUVtSqixngmrUhcUlIS/fr149///jcFCxb0dVgBw5uJIBkon2G/HOcO/cQCtwGo6k8iUggoCez1Ylw+Y/cKGJMze/bsoVSpUoSEhDB8+HAqVqxIzZo1fR1WwPHmHMFKIFxEKonIpbgmg+dlarMNiAEQkQigELDPizEZY/yAqjJ16lSuu+46Jk2aBMCdd95pScBLvNYjUNUUEekFLARCgHdUdZ2IDATiVHUe8CwwWUT64Bo2ekT9+C6Q860uZjWEjPFMUlIS3bp149tvv+Xmm2+mRQu3U4cmF3m1xISqLgAWZHquX4btBKCRN2PIS+dbXcxqCBmTvenTp/P4448TEhLChAkT6Natm9UHygNWayiX2TyAMRfummuu4ZZbbmH8+PGUK1fO1+EEDUsEF8lKSBtz4U6dOsWQIUM4c+YM/fv359Zbb+XWW2/1dVhBx/pcF8lKSBtzYVauXMmNN97Ia6+9RlJSkhWJ8yHrEeQCGw4yxnPHjx+nX79+jBo1irJlyzJv3jzuvPNOX4cV1KxHYIzJU5s3b+att96iW7durFu3zpJAPmA9AmOM1x0+fJhPP/2URx99lBo1apCYmEj58uWzf6PJE9YjMMZ41RdffEGNGjXo2rUrv//+O4AlgXzGegQ54O6GMbtSyBj39u3bx9NPP82sWbOIiori008/pXr16r4Oy7hhiSAH3N0wZlcKGXOu1NRUGjduzObNmxkwYAB9+/bl0ksv9XVYJguWCHLIrhAyJmu7d++mdOnShISEMGLECMLCwoiKivJ1WCYbNkdgjLloZ86cYeLEiVSrVo2JEycCcMcdd1gS8BPZJgIRKSwiL4nIBGe/qoi09n5oxhh/kJiYSExMDD169KBu3bq0atXK1yGZHPKkR/AOrkVmGjv7O4HBXovIGOM33n33Xa6//npWr17N5MmT+eabb6hcubKvwzI55EkiCFfVwcBpAFU9jvvVx4wxQaZChQq0atWKhIQEunbtioj9avBHnkwWn3JWDlMAEakEnPJqVMaYfOnkyZP897//5cyZMwwcOJCYmBhiYmJ8HZa5SJ70CP4DfAWUE5HpwHfAv70alTEm31mxYgU33ngjAwYMYNu2bVYkLoBkmwhU9UugA9AN+Ayop6rfeDswY0z+8Ndff/HMM8/QoEEDDh8+zPz585k2bZoNAwUQT64a+lpV96nqXFX9n6ruFZGv8yI4Y4zvbd26lXHjxtGjRw/WrVtHmzZtfB2SyWVZzhE4C84XAsqISFH+mSC+AqiQB7EZY3zk0KFDzJkzh65duxIZGUliYqKtGBbAztcjeAJYB1R3/kx7LAQmeD80Y4wvzJ07l8jISHr06JFeJM6SQGDLMhGo6ihVLQ+8qKoVVLW886ihqqPzMEZjTB7Yu3cvnTp1on379pQqVYrly5dbkbggke3lo6o6WkSqA5G4horSnp/lzcCMMXknNTWVRo0asW3bNgYNGsQLL7xAgQIFfB2WySPZJgIReQVoiWuIaCHQCvgBsERgjJ/buXMnV199NSEhIbz55puEhYURGRnp67BMHvPkPoKOQHNgl6p2BmoRZFVLZ63YRseJP6UvUm+Mvztz5gzjx4+nevXqTJjgmvK7/fbbLQkEKU9+oZ9Q1VQRSXGuHtoNBFwxEXeLzqRZsflPAOpXKm5rDxi/t3HjRrp168bSpUtp0aIFrVtbDclg50ki+EVEiuEqPhcHHAFWezUqH3C36EyatARwf327atb4t6lTp9KrVy8KFSrEO++8wyOPPGI3hpnzJwJx/YT0V9VDwFgRWQhcoaoBkQgy9gLSkoAtOmMCWVhYGK1bt2bs2LGULVvW1+GYfOK8iUBVVUTmAzc6+4l5ElUeydgLsCUnTSA6efIk//nPfwAYNGiQFYkzbnkyNPSziNQJlF5AZtYLMIHqxx9/JDY2lt9//50uXbqgqjYMZNzy5KqhxriSwQYRWS0iv4iIXycFuwrIBLJjx47Ru3dvGjduzPHjx/nqq6+YOnWqJQGTJU96BO0v9MNF5DbgTSAEmKKqQ9y0uRfoj2u9g19V9f4LPZ6nMg4J2XCQCTTbtm1j4sSJPPHEEwwePJiiRYv6OiSTz3lyZ/EfF/LBIhICjAVuBZKBlSIyT1UTMrQJB14CGqnqQREpfSHHuhA2JGQCycGDB/n444/p3r07kZGRJCUlcc011/g6LOMnPBkaulD1gERVTVLVU8CHQLtMbboBY1X1IICq7vViPMYEpM8++4zIyEgef/xxNmzYAGBJwOSINxPBtcD2DPvJznMZVQOqicgyEVnuDCWdQ0S6i0iciMTt27fPS+Ea4192795Nhw4d+Ne//sXVV1/Nzz//zHXXXefrsIwf8qhUhIiUw7WI/XciUhAIVdW/snubm+cyr20XCoQDzYBywPciEuXct/DPm1QnAZMAoqOjbX08E/RSU1Np0qQJ27dvZ/DgwTz33HNWJM5cME+KznUBegFXAlWAisA4oEU2b00GymfYLwfsdNNmuaqeBjaLyAZciWGlR9EbE2SSk5O55pprCAkJYcyYMVSqVMlKRZuL5snQ0FPATbhKS6CqGwFPJnVXAuEiUslZ7awTMC9Tm//hKmiHiJTENVSU5FnoxgSPM2fO8NZbb1G9enXGjx8PQOvWrS0JmFzhSSL425nsBdKvBsr2gmRVTcHVk1gIrAdmq+o6ERkoIm2dZguBAyKSAHwHPK+qB3J6EsYEst9//52mTZvy1FNP0bhxY+644w5fh2QCjCdzBMtE5AWgkIg0x7WE5XxPPlxVFwALMj3XL8O2As84D2NMJlOmTKFXr14UKVKE6dOn07lzZ7sxzOQ6T3oELwBHgd+B3sBi4GVvBmWMcalSpQp33nkn69ev56GHHrIkYLzCkx7B7bjuCh7v7WCMCXZ///03AwcOBGDw4ME0b96c5s2b+zgqE+g86RHcCySKyLsi0sqZIzDG5LJly5ZRu3Zt/vvf/7Jv3z5cI6fGeF+2icBZnrIa8DnQBUgSkQneDsyYYHH06FGefPJJmjRpwsmTJ1m4cCGTJ0+2YSCTZzy6s1hVTwJzgWm4Lgu914sxGRNUkpOTmTJlCk8++SRr166lZcuWvg7JBJlsE4GItBCRKcAfwIPAe8DV3g7MmEB24MCB9PsBIiIiSEpK4s033+Tyyy/3cWQmGHnSI+gBfAVEqOoDqjov430FxhjPqSpz5swhMjKSp556Kr1InC0baXzJkzmCe1R1jqqeyIuAjAlUu3bt4u6776ZDhw6UL1+euLg4KxJn8oUsLx8Vkf9T1ZtF5CBnF4sTXPeCFfd6dMYEiLQicTt27GDYsGH06dOH0FCPaj4a43Xn+0lMu3i5ZF4E4m2zVmxjbvwOgPTVyYzxtu3bt3PttdcSEhLC2LFjqVSpEtWqVfN1WMacJcuhIVU942xOVdXUjA9gat6El3vSlqcEbIlK43WpqamMGTPmrCJxrVq1siRg8iVP+qY1M+44N5TV9U443mXLU5q8sH79emJjY/npp59o3bo1d955p69DMua8suwRiMiLzvxATRH503kcBPaRqZCcMcZl0qRJ1K5dm40bNzJjxgy++OILKlSo4OuwjDmv8101NAwoBYxy/iwFlFTV4qr6fF4EZ4y/CQ8P56677iIhIYEHH3zQ7g42fuF8Q0NVVXWTiMwAaqQ9mfaDraprvBybMfneiRMn6N+/PyLCkCFDrEic8UvnSwR9gVhgrJvXFGjqlYiM8RNLly6la9eubNq0iR49eqCq1gMwfinLRKCqsc6fTfIuHGPyvyNHjtC3b1/Gjx9P5cqVWbx4MbfccouvwzLmgnlSa+hfIlLU2e4rIrNFpJb3QzMmf9q5cyfTpk3jmWeeYc2aNZYEjN/zpNZQf1U9KiINgTuBj4CJ3g3LmPxl//79jBs3DoDq1auzefNmRowYwWWXXebjyIy5eJ4kglTnzzuAcar6CVDQeyEZk3+oKh999BGRkZE8/fTTbNy4EYAyZcr4ODJjco8niWCXiIwFOgELRORSD99njF/buXMn7du3p1OnTlSsWJFVq1bZncEmIHlyZ/G9uNYtfktVD4rINbiuKDImYKWmptK0aVN27NjB8OHD6d27txWJMwEr259sVT0mIglAMxFpBnyvql96PTJjfGDr1q2UK1eOkJAQxo0bR+XKlalataqvwzLGqzy5aqgXMBuo4Dxmi8jj3g7MmLyUmprKyJEjiYiISC8S17JlS0sCJih40tftDtRT1WMAIjIY+BEY583AjMkrv/32G7Gxsfz888/ccccdtG/f3tchGZOnPJn0FeB0hv3TznPG+L0JEyZQp04dkpKSmDVrFvPmzaNcuXK+DsuYPOVJj2AGsFxEPsGVANoD070alTFellYOIiIigg4dOjB69GhKlSrl67CM8QlPJouHich3QFqpiR6qutK7YRnjHcePH6dfv36EhIQwdOhQbr75Zm6++WZfh2WMT3l6P8BJ53HC+dMYv7NkyRJq1qzJiBEjOHbsGKqa/ZuMCQKeXDX0MvABUBYoB8wSkZe8HZgxueXw4cM89thj6eWhv/32W8aOHWuVQo1xeDJH8CBwo6oeBxCR14FVwH+9GZgxuWXXrl28//77PPfccwwYMIAiRYr4OiRj8hVPhoa2cnbCCAWSPPlwEblNRDaISKKIZHk3sojcIyIqItGefK4x2dm3bx9vvfUW4CoSt2XLFt544w1LAsa44UkiOA6sE5EpIjIZWAscEpGRIjIyqzc5i9yPBVoDkcB9IhLppl1R4ClgxYWcgDEZqSqzZs0iIiKCZ599Nr1InF0RZEzWPBka+sJ5pFnu4WfXAxJVNQlARD4E2gEJmdr9B9f6yM95+LnGuLV9+3Z69uzJF198Qf369Zk6daoViTPGA55cPjr1Aj/7WmB7hv1koH7GBiJyA1BeVeeLSJaJQES647rDmQoVKlxgOCaQpaSk0KxZM3bv3s2oUaN48sknCQkJ8XVYxvgFb5ZTdHdJRvr1eiJyCTAKeCS7D1LVScAkgOjoaLvmz6TbsmUL5cuXJzQ0lIkTJ1K5cmUqV67s67CM8SveXFcgGSifYb8csDPDflEgClgiIluAm4B5NmFsPJGSksLw4cOJiIhIXzmsRYsWlgSMuQAe9whEpKCq5uRmspVAuIhUAnbgWtjm/rQXVfUwUDLD5y8BnlPVuBwcwwShNWvWEBsbS1xcHO3atePuu+/2dUjG+DVPbiirJyJrgU3Ofi0ReSu796lqCtALWAisB2ar6joRGSgibS8ybhOkxo0bx4033sjWrVv56KOP+Oyzz7jmmmt8HZYxfs2THsEYXOsV/w9AVX8VkeaefLiqLgAWZHquXxZtm3nymSY4pRWJi4qKolOnTowaNYqSJUtm/0ZjTLY8SQSXqOrWTLfjp2bV2Jjc9Ndff/HKK68QGhrKG2+8QdOmTWnatKmvwzImoHgyWbxdROoBKiIhIvI0sNHLcRnD4sWLuf766xk9ejQnT560InHGeIkniaAn8AyuZSr34Lq6p6c3gzLB7dChQ3Tt2pUWLVoQGhrK0qVLGTNmjBWJM8ZLPLmhbC+uK36MyRN79uzhww8/5MUXX+S1116jcOHCvg7JmICWbSJw6gud0ydX1e5eicgEpbRf/r179+a6665jy5YtNhlsTB7xZGjoG2Cx81gGlMYWpzG5RFV5//33iYyM5IUXXmDTpk0AlgSMyUOeDA19lHFfRGYAi7wWkQka27Zto0ePHnz55Zc0aNCAqVOnEh4e7uuwjAk6F1JrqBJQMbcDMcElrUjc3r17GTNmDI8//rgViTPGRzyZIzjIP3MElwB/AlkuMmPM+SQlJVGxYkVCQ0OZPHkyVapUISwszNdhGRPUzjtHIK7r9WoBpZzHVapaWVVn50VwJnCkpKQwdOhQIiMjGTt2LAAxMTGWBIzJB87bI1BVFZHPVPXGvArIBJ74+HhiY2NZvXo1d911Fx06dPB1SMaYDDy5auhnEanj9UhMQHr77bepW7cuO3bsYM6cOXz66aeULVvW12EZYzLIskcgIqFOBdHGQDcR+QP4C9eCM6qqlhxMltKKxNWsWZMHHniAkSNHUrx4cV+HZYxx43xDQz8DdYD2eRSLCQDHjh3j5ZdfpkCBAgwfPtyKxBnjB843NCQAqvqHu0cexWf8yNdff01UVBRvvfUWp0+ftiJxxviJ8/UISonIM1m9qKojvRCP8UMHDx7kmWeeYdq0aVx33XUsXbqUxo0b+zosY4yHztcjCAEux7W2sLuHMQDs3buXOXPm8NJLLxEfH29JwBg/c74ewS5VHZhnkRi/snv3bj744AP69OmTXiSuRIkSvg7LGHMBsp0jMCYjVWX69OlERkby0ksvpReJsyRgjP86XyKIybMojF/YsmULt912G4888giRkZHEx8dbkThjAkCWQ0Oq+mdeBmLyt5SUFJo3b87+/fsZO3YsPXr04JJLPLkf0RiT311I9VETRBITE6lUqRKhoaG88847VK5cmYoVrfisMYHEvtIZt06fPs3gwYOpUaNGepG45s2bWxIwJgBZj8CcY/Xq1cTGxhIfH0+HDh3o2LGjr0MyxniR9QjMWcaMGUO9evXYvXs3n376KbNnz6ZMmTK+DssY40WWCAxAejmIG264gYceeoiEhATuuusuH0dljMkLNjQU5I4ePcpLL71EwYIFGTFiBE2aNKFJkya+DssYk4esRxDEvvrqK6Kiohg3bhyqakXijAlSlgiC0IEDB3j44Ydp3bo1l112GcuWLWPkyJG4ViY1xgQbSwRB6MCBA3z22We8+uqr/PLLLzRo0MDXIRljfMiriUBEbhORDSKSKCJ93bz+jIgkiMgaEVksInaRupfs2rWL4cOHo6pUq1aNrVu3MnDgQAoWLOjr0IwxPua1RCAiIcBYoDUQCdwnIpGZmv0CRKtqTWAOMMxb8QQrVeWdd94hIiKCV199lcTERACuuuoqH0dmjMkvvNkjqAckqmqSqp4CPgTaZWygqt+p6nFndzlQzovxBJ3NmzfTsmVLYmNjqVWrFr/++qsViTPGnMObl49eC2zPsJ8M1D9P+1jgS3cviEh3oDtAhQoVciu+gJaSksItt9zCgQMHGD9+PN27d7ciccYYt7yZCNxdguL2+kQReRCIBm5297qqTgImAdrIXtoAABUTSURBVERHR9s1juexadMmKleuTGhoKO+++y5VqlShfPnyvg7LGJOPefMrYjKQ8TdQOWBn5kYi0gJ4GWirqie9GE9AO336NIMGDSIqKoq3334bgGbNmlkSMMZky5s9gpVAuIhUAnYAnYD7MzYQkRuAicBtqrrXi7EEtLi4OGJjY1mzZg2dOnXivvvu83VIxhg/4rUegaqmAL2AhcB6YLaqrhORgSLS1mn2BnA58LGIxIvIPG/FE6jefPNN6tevz/79+5k7dy4ffPABpUuX9nVYxhg/4tVaQ6q6AFiQ6bl+GbZbePP4gUxVERGio6OJjY1l2LBhFCtWzNdhGWP8kBWd8zNHjhzhxRdfpFChQowaNYpGjRrRqFEjX4dljPFjdj2hH1mwYAE1atRg0qRJhIaGWpE4Y0yusETgB/bv38+DDz5ImzZtuPLKK/nxxx954403rEicMSZXWCLwAwcPHuTzzz/ntddeY/Xq1dSvf7778owxJmdsjiCf2rFjBzNnzuT5558nPDycrVu32mSwMcYrrEeQz6gqkydPJjIykv79+/PHH38AWBIwxniNJYJ85I8//iAmJobu3btTp04d1qxZQ9WqVX0dljEmwNnQUD6RkpJCTEwMf/75JxMnTqRr165WJM4YkycsEfjYhg0bqFKlCqGhoUyfPp0qVapQrpxV4zbG5B37yukjp06dYsCAAVx//fWMHTsWgJtvvtmSgDEmz1mPwAd+/vlnYmNj+e2337j//vt54IEHfB2SMSaIWY8gj40ePZoGDRqk3xswc+ZMSpYs6euwjDFBzBJBHkkrB1GvXj26devGunXruOOOO3wclTHG2NCQ1x0+fJgXXniBwoULM3r0aBo2bEjDhg19HZYxxqSzHoEXff7550RGRjJlyhQKFixoReKMMfmSJQIv2LdvH/fffz9t27alRIkSLF++nKFDh1qROGNMvmSJwAsOHz7MggULGDBgAHFxcdStW9fXIRljTJZsjiCXbN++nffff5++fftStWpVtm7dypVXXunrsIwxJlvWI7hIZ86cYcKECdSoUYNBgwalF4mzJGCM8ReWCC7Cpk2buOWWW+jZsyf16tVj7dq1ViTOGON3bGjoAqWkpHDrrbdy6NAhpk6dyqOPPmqTwcYYv2SJIIfWr19PeHg4oaGhzJgxgypVqnDNNdf4OiwTwE6fPk1ycjJ///23r0MxfqBQoUKUK1eOAgUKePweSwQeOnnyJIMHD2bw4MG88cYbPP300zRp0sTXYZkgkJycTNGiRQkLC7NepzkvVeXAgQMkJydTqVIlj99nicADy5cvJzY2loSEBDp37kznzp19HZIJIn///bclAeMREaFEiRLs27cvR++zyeJsjBgxgoYNG3L06FEWLFjAe++9R4kSJXwdlgkylgSMpy7kZ8USQRbOnDkDQIMGDejRowe//fYbrVu39nFUxhiT+ywRZHLo0CFiY2Pp3bs3AA0bNmTcuHFcccUVPo7MGN+5/PLL07cXLFhAeHg427Zty7Pj33PPPSQlJeXZ8XJq8+bN1K9fn/DwcDp27MipU6fOaXPq1CkeffRRrr/+emrVqsWSJUvOadO2bVuioqLS9//8809uvfVWwsPDufXWWzl48CAA8+fP57XXXsu1+C0RZPC///2PyMhIpk+fTtGiRa1InDGZLF68mCeffJKvvvqKChUqePSelJSUizrmunXrSE1NpXLlyh6/JzU19aKOmVMvvvgiffr0YdOmTVx11VVMnTr1nDaTJ08GYO3atSxatIhnn302feQB4NNPPz0r4QIMGTKEmJgYNm3aRExMDEOGDAGgTZs2zJs3j+PHj+dK/DZZDOzdu5devXrx8ccfU7t2bebPn0+dOnV8HZYx5xjw+ToSdh7J1c+MvOYKXruzRrbtvv/+e7p168aCBQuoUqUK4Cqw2KNHj/TewejRo2nUqBH9+/dn586dbNmyhZIlSzJ48GA6d+7MX3/9BcDbb79Nw4YN2bVrFx07duTIkSOkpKQwfvz4c67GmzlzJu3atUvf79mzJytXruTEiRPcc889DBgwAICwsDC6dOnC119/Ta9evahbty5PPPEE+/bto0iRIkyePJnq1avz+eefM2jQIE6dOkWJEiWYOXMmZcqUueC/P1Xl22+/ZdasWQA8/PDD9O/fn549e57VLiEhgZiYGABKly5NsWLFiIuLo169ehw7doyRI0cyadIk7r333vT3zJ07N73n8PDDD9OsWbP0ApbNmjVj/vz5Z7W/UJYIgCNHjrBo0SJef/11nn/++Rxdf2tMMDh58iTt2rVjyZIlVK9ePf353r1706dPHxo3bsy2bdto1aoV69evB2DVqlX88MMPFC5cmOPHj7No0SIKFSrEpk2buO+++4iLi2PWrFm0atWKl19+mdTUVLffcJctW8Z9992Xvv/6669TvHhxUlNTiYmJYc2aNdSsWRNwXUP/ww8/ABATE8OECRMIDw9nxYoVPP7443z77bc0btyY5cuXIyJMmTKFYcOGMWLEiLOOuWHDBjp27Oj272LJkiUUK1Ysff/AgQMUK1aM0FDXr9Ny5cqxY8eOc95Xq1Yt5s6dS6dOndi+fTurVq1i+/bt1KtXj1dffZVnn32WIkWKnPWePXv2ULZsWQDKli3L3r1701+Ljo7m+++/t0RwMbZt28aMGTP497//TdWqVdm2bRtFixb1dVjGnJcn39y9oUCBAjRs2JCpU6fy5ptvpj//zTffkJCQkL5/5MgRjh49CrjGuwsXLgy4borr1asX8fHxhISEsHHjRgDq1q1Lly5dOH36NO3bt6d27drnHHvXrl2UKlUqfX/27NlMmjSJlJQUdu3aRUJCQnoiSPvlfezYMX788Uc6dOiQ/r6TJ08CrvsyOnbsyK5duzh16pTb6+2vu+464uPjPfq7cTeE7O7KnS5durB+/Xqio6OpWLEiDRs2JDQ0lPj4eBITExk1ahRbtmzx6Jjg6lXs3LnT4/bn49U5AhG5TUQ2iEiiiPR183pBEfnIeX2FiIR5Mx5wXQ00btw4atSoweDBg9OLxFkSMCZrl1xyCbNnz2blypUMHjw4/fkzZ87w008/ER8fT3x8PDt27Ej/v3TZZZeltxs1ahRlypTh119/JS4uLn0ytWnTpixdupRrr72Wzp078957751z7MKFC6ffVb1582aGDx/O4sWLWbNmDW3atDnrjuu0Y545c4ZixYqlxxUfH5/eU3nyySfp1asXa9euZeLEiW7v2N6wYQO1a9d2+zh06NBZbUuWLMmhQ4fS50KSk5PdVhsIDQ1l1KhRxMfHM3fuXA4dOkR4eDg//fQTq1atIiwsjMaNG7Nx40aaNWsGQJkyZdi1axfgSoilS5dO/7y///47PdFeLK8lAhEJAcYCrYFI4D4RiczULBY4qKpVgVHAUG/FA3DixHGaNWvGE088QYMGDVi3bp0ViTPGQ0WKFGH+/PnMnDkzfTK0ZcuWvP322+ltsvoWffjwYcqWLcsll1zCjBkz0idzt27dSunSpenWrRuxsbGsXr36nPdGRESQmJgIuHocl112GVdeeSV79uzhyy+/dHu8K664gkqVKvHxxx8Drm/tv/76a3os1157LQDTp093+/60HoG7R8ZhIXB9+2/evDlz5sxJ/8yMcxppjh8/nj5HsmjRIkJDQ4mMjKRnz57p8yk//PAD1apVS58XaNu2bXqMmT9348aNZ11hdDG82SOoBySqapKqngI+BDL/7bQD0v4l5gAx4qU7Z1SVNWvWsHbtWt59910WLlxIWFiYNw5lTMAqXrw4X331FYMGDWLu3LmMGTOGuLg4atasSWRkJBMmTHD7vscff5zp06dz0003sXHjxvRv7kuWLKF27drccMMNfPLJJ+mXbWfUpk2b9F+MtWrV4oYbbqBGjRp06dKFRo0aZRlrWsKqVasWNWrUYO7cuQD079+fDh060KRJE0qWLHmRfyMuQ4cOZeTIkVStWpUDBw4QGxsLwLx58+jXrx/guiilTp06REREMHToUGbMmJHt5/bt25dFixYRHh7OokWL6Nv3n4GV7777jjZt2uRK/OKtSyRF5B7gNlXt6ux3Buqraq8MbX5z2iQ7+384bfZn+qzuQHeAChUq3Lh169YcxzPg83Xs3LmT/m2j0idfjPEH69evJyIiwtdh+MyJEydo3rw5y5YtIyQkxNfh5At79uzh/vvvZ/HixW5fd/czIyKrVDXaXXtvTha7+2afOet40gZVnQRMAoiOjr6gzOWaZPPNRJsx5sIVLlyYAQMGsGPHDo/vXQh027ZtO+dKp4vhzUSQDJTPsF8OyDzFndYmWURCgSuBP70YkzHGD7Vq1crXIeQrub0OujfnCFYC4SJSSUQuBToB8zK1mQc87GzfA3yrdjuvMeew/xbGUxfys+K1RKCqKUAvYCGwHpitqutEZKCItHWaTQVKiEgi8AxwziWmxgS7QoUKceDAAUsGJltp6xEUKlQoR+/z2mSxt0RHR2tcXJyvwzAmz9gKZSYnslqhzFeTxcaYXFCgQIEcrTZlTE5Z9VFjjAlylgiMMSbIWSIwxpgg53eTxSKyD8j5rcUuJYH92bYKLHbOwcHOOThczDlXVNVS7l7wu0RwMUQkLqtZ80Bl5xwc7JyDg7fO2YaGjDEmyFkiMMaYIBdsiWCSrwPwATvn4GDnHBy8cs5BNUdgjDHmXMHWIzDGGJOJJQJjjAlyAZkIROQ2EdkgIokick5FUxEpKCIfOa+vEJGwvI8yd3lwzs+ISIKIrBGRxSJS0Rdx5qbszjlDu3tEREXE7y819OScReRe5996nYjMyusYc5sHP9sVROQ7EfnF+fm+3Rdx5hYReUdE9jorOLp7XURkjPP3sUZE6lz0QVU1oB5ACPAHUBm4FPgViMzU5nFggrPdCfjI13HnwTk3B4o42z2D4ZyddkWBpcByINrXcefBv3M48AtwlbNf2tdx58E5TwJ6OtuRwBZfx32R59wUqAP8lsXrtwNf4lrh8SZgxcUeMxB7BPWARFVNUtVTwIdAu0xt2gHTne05QIyIuFs2019ke86q+p2qHnd2l+NaMc6fefLvDPAfYBgQCDWcPTnnbsBYVT0IoKp78zjG3ObJOStwhbN9JeeuhOhXVHUp51+psR3wnrosB4qJyEUtxB6IieBaYHuG/WTnObdt1LWAzmGgRJ5E5x2enHNGsbi+UfizbM9ZRG4Ayqvq/LwMzIs8+XeuBlQTkWUislxEbsuz6LzDk3PuDzwoIsnAAuDJvAnNZ3L6/z1bgbgegbtv9pmvkfWkjT/x+HxE5EEgGrjZqxF533nPWUQuAUYBj+RVQHnAk3/nUFzDQ81w9fq+F5EoVT3k5di8xZNzvg+YpqojRKQBMMM55zPeD88ncv33VyD2CJKB8hn2y3FuVzG9jYiE4upOnq8rlt95cs6ISAvgZaCtqp7Mo9i8JbtzLgpEAUtEZAuusdR5fj5h7OnP9lxVPa2qm4ENuBKDv/LknGOB2QCq+hNQCFdxtkDl0f/3nAjERLASCBeRSiJyKa7J4HmZ2swDHna27wG+VWcWxk9le87OMMlEXEnA38eNIZtzVtXDqlpSVcNUNQzXvEhbVfXndU49+dn+H64LAxCRkriGipLyNMrc5ck5bwNiAEQkAlci2JenUeatecBDztVDNwGHVXXXxXxgwA0NqWqKiPQCFuK64uAdVV0nIgOBOFWdB0zF1X1MxNUT6OS7iC+eh+f8BnA58LEzL75NVdv6LOiL5OE5BxQPz3kh0FJEEoBU4HlVPeC7qC+Oh+f8LDBZRPrgGiJ5xJ+/2InIB7iG9ko68x6vAQUAVHUCrnmQ24FE4Djw6EUf04//vowxxuSCQBwaMsYYkwOWCIwxJshZIjDGmCBnicAYY4KcJQJjjAlylghMviUiqSISn+ERdp62YVlVa8xrIhItImOc7WYi0jDDaz1E5KE8jKW2v1fjNN4XcPcRmIByQlVr+zqInHJuWku7ca0ZcAz40XltQm4fT0RCnZpZ7tTGVVJkQW4f1wQO6xEYv+J88/9eRFY7j4Zu2tQQkZ+dXsQaEQl3nn8ww/MTRSTEzXu3iMhQp93PIlLVeb6iuNZxSFvPoYLzfAcR+U1EfhWRpc5zzURkvtOD6QH0cY7ZRET6i8hzIhIhIj9nOq81zvaNIvJ/IrJKRBa6qywpItNEZKSIfAcMFZF6IvKjuGry/ygi1zl34g4EOjrH7ygil4mr3v1Kp627iq0m2Pi69rY97JHVA9edsfHO4zPnuSJAIWc7HNfdpQBhOPXbgbeAB5ztS4HCQATwOVDAeX4c8JCbY24BXna2HwLmO9ufAw87212A/znba4Frne1izp/NMryvP/Bchs9P33fOq7Kz/SLwCq47SH8ESjnPd8R1N23mOKcB84EQZ/8KINTZbgF84mw/Aryd4X2DgQfT4gU2Apf5+t/aHr592NCQyc/cDQ0VAN4Wkdq4EkU1N+/7CXhZRMoBn6rqJhGJAW4EVjolNgoDWdVc+iDDn6Oc7QbAv5ztGbjWOABYBkwTkdnApzk5OVyF0u4FhuD6hd8RuA5XsbxFTpwhQFZ1ZD5W1VRn+0pgutP7UZySBG60BNqKyHPOfiGgArA+h7GbAGKJwPibPsAeoBauoc1zFpxR1VkisgJoAywUka64SvdOV9WXPDiGZrF9ThtV7SEi9Z1jxTsJylMf4ar99Knro3STiFwPrFPVBh68/68M2/8BvlPVu5whqSVZvEeAu1V1Qw7iNAHO5giMv7kS2KWuWvOdcX1jPouIVAaSVHUMrkqNNYHFwD0iUtppU1yyXre5Y4Y/f3K2f+Sf4oQPAD84n1NFVVeoaj9gP2eXBwY4iqsk9jlU9Q9cvZpXcSUFcJWNLiWuuvqISAERqZFFnBldCexwth85z/EXAk+K090QV1VaE+QsERh/Mw54WESW4xoW+stNm47AbyISD1THtaxfAq4x+K+dSdlFQFbL+xV0ehS9cfVAAJ4CHnXe29l5DeANEVnrXLq6FNeauhl9DtyVNlns5lgfAQ/yTz39U7hKow8VkV9xzSOcMyHuxjDgvyKyjLOT43dAZNpkMa6eQwFgjRPzfzz4bBPgrPqoMRmIaxGbaFXd7+tYjMkr1iMwxpggZz0CY4wJctYjMMaYIGeJwBhjgpwlAmOMCXKWCIwxJshZIjDGmCD3/7So9xhQnTnUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEWCAYAAABFSLFOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5xVdb3/8ddbQPECgoIeFBFUUiEFFTFvhYcU019ipaGd1MLjNa20zkmzkixNO6Xm0fJgerwcSUkryDTDC+VdBgVMPAreYGCOEqioqAh8fn+s7+Bi2DOzh1l7NjO8n4/Hfsy6fNd3ffZisz/7u75rfZciAjMzs6JsVO0AzMysY3FiMTOzQjmxmJlZoZxYzMysUE4sZmZWKCcWMzMrlBOLbRAkDZJUU+041pWkqZL+tZF135X067aOqS1ICkm7lFn2DEmvSXpH0tYl1u8p6dHio7SGnFg6KEmvSHov/Sf7P0k3StqiQZkDJD0g6W1Jb0n6o6RBDcp0l3SlpHmprrlpvlfbvqNW+xHws2oHUQkRcUlElEw6rSGpf/pi79xEmXGS/qfofbeUpC7A5cBhEbFFRCxumJQiYhbwpqTPVi3QDYQTS8f22YjYAhgK7AWcX79C0v7AX4BJwHbAAGAm8IiknVKZjYH7gcHA4UB34ABgMTC8UkE39UW2jvX1AQ4B/lBkvbZe2RboCjzbTLlbgdMqH84GLiL86oAv4BXg07n5nwJ/ys0/BPyyxHb3ADen6X8FXgO2aMF+BwNTgCVp2++m5TcCP86VGwHUNoj3O8As4APge8AdDer+BXBVmt4SuB6oAxYAPwY6NRLTicB9ufkxwDu51wfA1Fy9NwOLgFdTHBuldRul+VeB11O5LdO6/kAAXwXmA28ApwP7pvf0JnB1g7jGAs+lsvcCO+bWHQr8L/AWcDXwV+BfG3l/44D/aRDHScA84B/ABU38ex0JPA0sTXGPy62bl+qqP077N9j2cGA58GFaPzMt3w6YnD4Dc4FTGsR6B3A78DbwFDCkifgC2CVNb0LW6pyXPlvXApsCHwPezcX6APC3NP9uWjYm1bE98B6wSbX/j3bkV9UD8KtC/7C5xAL0BZ4BfpHmNwNWAoeU2O6rQF2avg24qQX77Eb2Rf8tsl+P3YD90robaT6xzAB2SF8WOwLLgO5pfadU9yfS/B+A/wI2B7YBngROaySu/wCuaWRdd7Iv99PS/M1krbhu6Uv6BeDktG5s+qLcCdgC+B1wS1rXP32RXZve+2HA+ynObdIX2uvAp1L5o1NduwOdyRLWo2ldL7Iv+mOALsA5wApalliuS8dxCFni3L2RbUcAe5AlzT3JvrCPblBX5yb+zVfvO7fsr8Av03EYSpakR+bKf5h7b98GXga6NFJ/PrFcSZawtkr/Pn8EftJYrPltG9S5FNiz2v9HO/Kr6gH4VaF/2OyL+h2yX4VBdkqrR1rXNy3brcR2hwMfpukpwKUt2OfxwNONrLuR5hPL2AbbPAycmKYPBV5M09umL8tNG+z7wUb2fV2p95G+TO8CfpXmO6V6B+XKnMZHrZn7gTNz63ZNX5Kdc19s2+fWLyb9Uk7zdwLfTNP3kBJWLpZlZAn1RODx3DoBtbQssfTNrX8SOK7Mf8MrgSsa1FV2YiH7YbAS6JZb9hPgxlz5/HvbiOwHw8GN1B/ALukYvAvsnFu3P/ByY7HSeGJZAHyyLf4fbqgv97F0bEdHRDeyL/HdyH4JQ3bqZRXQp8Q2fchOn0D2xViqTGN2AF5cp0gz8xvMTyBLGABfSvOQffl2AeokvSnpTbLWyzaN1PsG2S/chi5Oy7+e5nsBG5Od6qr3KllrA7JTPA3XdSZLdPVey02/V2K+/gKKHYFf5OJfQvbluX3az+pjEdm3YcNj05z/y00vy+13DZL2k/SgpEWS3iI7fdeaCzO2A5ZExNu5ZfljCGu+t1VkSXO7ZurtTdbSnp47Zn9Oy1uqG9mpSasQJ5YNQET8lazF8LM0/y7wGHBsieJfJPtlDnAfMErS5mXuaj6wcyPr3iX7Yqj3T6VCbTD/W2CEpL7A5/goscwna1n0ioge6dU9IgY3su9ZZOfhV5N0HFnSOiYiPkyL/0HWAtkxV7Qf2S9cgIUl1q1gzeRRrvlkp9965F6bRsSjZL/gd8jFqvx8wSaQnV7aISK2JDuVp7Su4b9HKQ3LLAS2kpRP5PljCGu+t43IWtALm9nPP8gS8+Dc8doysotTyiZpO7IfD8+3ZDtrGSeWDceVwKGShqb584CTJH1dUjdJPSX9mOz0wg9TmVvIvgDvlLSbpI0kbZ3umziixD7uAv5J0jclbZLq3S+tmwEcIWkrSf8EfLO5gCNiETAV+G+yUx7PpeV1ZFe0/TxdDr2RpJ0lfaqRqqYAe0vqCiBpL+A/yVp0i3L7WwlMBC5Ose8InAvUX077G+AcSQPSpduXALdHxIrm3ksJ1wLnSxqcYtpSUn2i/xMwWNLn0xVyX6d0Ii5CN7IWxvuShpO1DOstImvZ7tTE9q8B/VOCICLmA48CP5HUVdKewMlkV2PV2yf33r5J9iPh8aaCTC2b64ArJG0DIGl7SaOaia1h7COAByLig6b2Z63jxLKBSF+gNwPfT/MPA6OAz5P9Qn6V7JLkgyJiTirzAfBpsquTppB1ej5JdqrkiRL7eJusL+SzZKdi5pBd5gtZkppJ1pfyF7KrgsoxIcUwocHyE8l+ec4mO9V1B42ctouI18iuFBqdFo0GegIPp3tz3pF0T1p3Nlnr6iWyPp4JwA1p3Q3pffyNrMP5/VS+xSLi98BlwG2SlgJ/Bz6T1v2DrDV5KdnpyIHAI+uynzKcCVwk6W3gB2SJtT7GZWSnCx9Jp58+UWL736a/iyU9laaPJ+vzWAj8HrgwIqbktplEdmXeG8AJwOdzrcamfIfsgofH0zG7j6yfqzHjgJtS7F9My/6FLKlbBSk7fWvWsaUbP28Choc/9FUjaRxZh/qXq7DvPYDxEbF/W+97Q1PojWhm66uImE12T4ltoCLiGbJTvVZhFTsVJukGSa9L+nsj6yXpqjREyCxJe+fWnSRpTnqdVKkYzcyseBU7FSbpk2T3UdwcER8vsf4IsvPTRwD7kd28t5+krYAaYBjZFSfTgX0i4o2KBGpmZoWqWIslIv5Gdm1+Y0aTJZ2IiMeBHmlMp1HAlIhYkpLJFLKb9szMrB2oZh/L9qx501dtWtbY8rVIOhU4FWDzzTffZ7fddqtMpGZmHdT06dP/ERHrcqNpo6qZWFRiWTSxfO2FEeOB8QDDhg2Lmpp2+7gNM7OqkPRq86Vappr3sdSy5t3E9XffNrbczMzagWomlsnAienqsE8Ab6U7qu8FDkt3gvckGyX23irGaWZmLVCxU2GSfkM2fEIvSbXAhWQDBxIR1wJ3k10RNpdskLyvpnVLJP0ImJaquigimroIwMzM1iMVSywRcXwz6wP4WiPrbuCjYTTMrBEffvghtbW1vP/++9UOxdZzXbt2pW/fvnTp0qXi+/Kd92btWG1tLd26daN///5kgyCbrS0iWLx4MbW1tQwYMKDi+/MglGbt2Pvvv8/WW2/tpGJNksTWW2/dZi1bJxazds5JxcrRlp8TJxYzMyuUE4uZmRXKicXMWmWLLT56OvDdd9/NwIEDmTdvXpvt/5hjjuGll15qs/211Msvv8x+++3HwIEDGTNmDMuXL1+rzPLly/nqV7/KHnvswZAhQ5g6depaZY466ig+/vGPxvNdsmQJhx56KAMHDuTQQw/ljTeycXrvuusuLrzwwoq9n3I4sZhZIe6//37OPvts/vznP9OvX7+ytlmxYl2e6vyRZ599lpUrV7LTTk09PXlNK1eubNU+W+o73/kO55xzDnPmzKFnz55cf/31a5W57rrrAHjmmWeYMmUK3/rWt1i1atXq9b/73e/WSOAAl156KSNHjmTOnDmMHDmSSy+9FIAjjzySyZMns2zZsgq+q6b5cmOzDuKHf3yW2QuXFlrnoO26c+FnBzdb7qGHHuKUU07h7rvvZueddwZg0aJFnH766atbL1deeSUHHngg48aNY+HChbzyyiv06tWLSy65hBNOOIF3330XgKuvvpoDDjiAuro6xowZw9KlS1mxYgW/+tWvOPjgg9fY76233sro0aNXz59xxhlMmzaN9957j2OOOYYf/vCHAPTv35+xY8fyl7/8hbPOOot9992Xr33tayxatIjNNtuM6667jt12240//vGP/PjHP2b58uVsvfXW3HrrrWy77bbrfPwiggceeIAJE7Ina5900kmMGzeOM844Y41ys2fPZuTIkQBss8029OjRg5qaGoYPH84777zD5Zdfzvjx4/niF7+4eptJkyatbtmcdNJJjBgxgssuuwxJjBgxgrvuumuN8m3JicXMWuWDDz5g9OjRTJ06lfwI49/4xjc455xzOOigg5g3bx6jRo3iueeeA2D69Ok8/PDDbLrppixbtowpU6bQtWtX5syZw/HHH09NTQ0TJkxg1KhRXHDBBaxcubLkL/BHHnmE44//6F7siy++mK222oqVK1cycuRIZs2axZ577glkNwg+/PDDAIwcOZJrr72WgQMH8sQTT3DmmWfywAMPcNBBB/H4448jiV//+tf89Kc/5ec///ka+3z++ecZM2ZMyWMxdepUevTosXp+8eLF9OjRg86ds6/avn37smDBgrW2GzJkCJMmTeK4445j/vz5TJ8+nfnz5zN8+HC+//3v861vfYvNNttsjW1ee+01+vTpA0CfPn14/fXXV68bNmwYDz30kBOLmbVOOS2LSujSpQsHHHAA119/Pb/4xS9WL7/vvvuYPXv26vmlS5fy9ttvA1l/waabbgpkowecddZZzJgxg06dOvHCCy8AsO+++zJ27Fg+/PBDjj76aIYOHbrWvuvq6ujd+6MR3ydOnMj48eNZsWIFdXV1zJ49e3ViqU8G77zzDo8++ijHHnvs6u0++OADILvhdMyYMdTV1bF8+fKSNxPuuuuuzJgxo6xjU+pBiqUu+x07dizPPfccw4YNY8cdd+SAAw6gc+fOzJgxg7lz53LFFVfwyiuvlLVPyFo9CxdWb+xeJxYza5WNNtqIiRMn8ulPf5pLLrmE7373uwCsWrWKxx57bHUCydt8881XT19xxRVsu+22zJw5k1WrVtG1a1cAPvnJT/K3v/2NP/3pT5xwwgn827/9GyeeeOIa9Wy66aarb/p7+eWX+dnPfsa0adPo2bMnX/nKV9a4IbB+n6tWraJHjx4lk8PZZ5/Nueeey1FHHcXUqVMZN27cWmVa0mLp1asXb775JitWrKBz587U1tay3XbbrbVd586dueKKK1bPH3DAAQwcOJC//vWvTJ8+nf79+7NixQpef/11RowYwdSpU9l2222pq6ujT58+1NXVsc0226ze/v333y953NuKO+/NrNU222wz7rrrLm699dbVndOHHXYYV1999eoyjf3Kf+utt+jTpw8bbbQRt9xyy+rO9VdffZVtttmGU045hZNPPpmnnnpqrW1333135s6dC2Qtos0335wtt9yS1157jXvuuafk/rp3786AAQP47W9/C2StipkzZ66OZfvts+cK3nTTTSW3r2+xlHrlkwpkrZNDDjmEO+64Y3Wd+T6hesuWLVvdxzRlyhQ6d+7MoEGDOOOMM1b3Rz388MN87GMfW92vctRRR62OsWG9L7zwwhpXkLU1JxYzK8RWW23Fn//8Z3784x8zadIkrrrqKmpqathzzz0ZNGgQ1157bcntzjzzTG666SY+8YlP8MILL6xuWUydOpWhQ4ey1157ceedd/KNb3xjrW2PPPLI1V+0Q4YMYa+99mLw4MGMHTuWAw88sNFY6xPgkCFDGDx4MJMmTQJg3LhxHHvssRx88MH06tWrlUckc9lll3H55Zezyy67sHjxYk4++WQAJk+ezA9+8AMAXn/9dfbee2923313LrvsMm655ZZm6z3vvPOYMmUKAwcOZMqUKZx33nmr1z344IMceeSRhcS/LlTqHGB75CdI2oboueeeY/fdd692GFXz3nvvccghh/DII4/QqVOnaoezXnjttdf40pe+xP3337/WulKfF0nTI2JYkTG4xWJm7damm27KD3/4w5JXWm2o5s2bt9aVbG3Nnfdm7VxEbNADUY4aNaraIaxX9t1335LL2/LslFssZu1Y165dWbx4cZt+aVj7U/88lvor7irNLRazdqxv377U1tayaNGiaodi67n6J0i2BScWs3asS5cubfJEQLOW8KkwMzMrlBOLmZkVyonFzMwK5cRiZmaFcmIxM7NCObGYmVmhnFjMzKxQTixmZlaoiiYWSYdLel7SXEnnlVi/o6T7Jc2SNFVS39y6lZJmpNfkSsZpZmbFqdid95I6AdcAhwK1wDRJkyNidq7Yz4CbI+ImSf8M/AQ4Ia17LyLWfhapmZmt1yrZYhkOzI2IlyJiOXAb0PDRaYOA+ocGPFhivZmZtTOVTCzbA/Nz87VpWd5M4Atp+nNAN0lbp/mukmokPS7p6ArGaWZmBapkYin1gIiGY3t/G/iUpKeBTwELgBVpXb/0VLMvAVdK2nmtHUinpuRT49FdzczWD5VMLLXADrn5vsDCfIGIWBgRn4+IvYAL0rK36telvy8BU4G9Gu4gIsZHxLCIGNa7d++KvAkzM2uZSiaWacBASQMkbQwcB6xxdZekXpLqYzgfuCEt7ylpk/oywIFAvtPfzMzWUxVLLBGxAjgLuBd4DpgYEc9KukjSUanYCOB5SS8A2wIXp+W7AzWSZpJ16l/a4GoyMzNbT6mjPNJ02LBhUVNTU+0wzMzaFUnTU392YXznvZmZFcqJxczMCuXEYmZmhXJiMTOzQjmxmJlZoZxYzMysUE4sZmZWKCcWMzMrlBOLmZkVyonFzMwK5cRiZmaFcmIxM7NCObGYmVmhnFjMzKxQTixmZlYoJxYzMyuUE4uZmRXKicXMzArlxGJmZoVyYjEzs0I5sZiZWaGcWMzMrFBOLGZmVignFjMzK5QTi5mZFarZxCJpU0nnS7o2ze8i6TOVD83MzNqjclosNwACDkrzC4FLKhaRmZm1a+UkloERcQnwIUBELCNLNGZmZmspJ7Esl9QVCABJA4Dl5VQu6XBJz0uaK+m8Eut3lHS/pFmSpkrqm1t3kqQ56XVSme/HzMyqrJzE8iPgz0BfSTcBDwLfbW4jSZ2Aa4DPAIOA4yUNalDsZ8DNEbEncBHwk7TtVsCFwH7AcOBCST3LekdmZlZVzSaWiLgHOBY4Bfg9MDwi7iuj7uHA3Ih4KSKWA7cBoxuUGQTcn6YfzK0fBUyJiCUR8QYwBTi8jH2amVmVlXNV2F8iYlFETIqIP0TE65L+Ukbd2wPzc/O1aVneTOALafpzQDdJW5e5LZJOlVQjqWbRokVlhGRmZpXWaGKRtLGk7sC2krpJ6p5efYF+ZdRdqoM/Gsx/G/iUpKeBTwELgBVlbktEjI+IYRExrHfv3mWEZGZmlda5iXVfA84FtgGe5aMv+6XAtWXUXQvskJvvS3ap8moRsRD4PICkLYAvRMRbkmqBEQ22nVrGPs3MrMoabbFExBURsQPwnYjoFxE7pNfgiLiyjLqnAQMlDZC0MXAcMDlfQFIvSfUxnE92zwzAvcBhknqmTvvD0jIzM1vPNdViASAirpS0G1lHe9fc8gnNbLdC0llkCaETcENEPCvpIqAmIiaTtUp+IimAv5G1koiIJZJ+RJacAC6KiCUtfndmZtbmFLFW18WaBaTvkbUYdiNLEqOAhyPi85UPr3zDhg2LmpqaaodhZtauSJoeEcOKrLOc+1jGAIcAdRFxAjCEMlo6Zma2YSonsbwXESuBFZK6Af8H7FTZsMzMrL0qp+XxtKQeZB3rNWRXhT1V0ajMzKzdajKxSBIwLiLeBK6RdC/QPSKcWMzMrKQmT4VF1rN/V25+rpOKmZk1pZw+licl7V3xSMzMrEMop4/lIOAUSS8C75LdgR8R4WRjZmZrKSexHF3xKMzMrM1NeGJeReot5877FyuyZzMzq6pJMxZUpN5y+ljMzMzK5sRiZmaFKiuxSOor6ZA0vYmkzSsblpmZtVflPEFyLNlw979Oi3YEJlUyKDMza7/KuSrs62TPr38CICJekLRNRaMyM1vPTHhiXsU6u6tldt3SitRbzqmw9yNief2MpE6UfnSwmVmHNWnGgop9EVfLoD7dK1JvOS2WRyT9O9A19bN8jdwwL2ZmG4pBfbpz+2n7VzuMQk08vfg6y2mx/DvwNvC/wDeA+4ELig/FzMw6gnJaLEcAv46IX1U6GDMza//KabF8EZgr6b8ljUp9LGZmZiWVM6TLCZI2AY4ExgLjJd0TERU4M2dm7V1HvHoKsiuoKtXZ3dGUdYNkRHxAdu/KjcA0slaMmdlaOuLVU5B13I8eun21w2gXmm2xSPo0cBzwaeAR4GbgSxWOy8zasY549ZSVr5zO+9OB24CzI+K9CsdjZmbtXDl9LMe0RSBmZtYxNJpYJP01Ij4l6Q0g8qvIniC5VcWjMzOzdqepFssh6W+vtgjEzMw6hkavCouIVWny+ohYmX8B17dNeGZm1t6Uc7nxnvmZdIPkvuVULulwSc9LmivpvBLr+0l6UNLTkmZJOiIt7y/pPUkz0uvacvZnZmbV11Qfy3eA84BukpbULybrb2m2xZIS0DXAoUAtME3S5IiYnSv2PWBiRPxK0iDgbqB/WvdiRAxt4fsxM7Mqa6rF8lOgN3BF+tsb6BURW0XEv5VR93BgbkS8lIbdvw0Y3aBMAPW3sm4JLGxJ8GZmtv5pqvN+l4iYI+kWYHD9Qil7FEtEzGqm7u2B+bn5WmC/BmXGAX+RdDawOdlNmPUGSHoaWAp8LyIeargDSacCpwL069evmXDMzKwtNJVYzgNOJjud1VAAn2ym7lIPA4sG88cDN0bEzyXtD9wi6eNAHdAvIhZL2gf4g6TBEbHGOBERMR4YDzBs2LCGdZuZWRU0mlgi4uT09+B1rLsW2CE335e1T3WdDBye9vOYpK5kp9teBz5Iy6dLehH4GFCzjrGYmVkbafaqMEmfl9QtTZ8naaKkIWXUPQ0YKGmApI3Jxhub3KDMPGBkqnt3oCuwSFLv+uH5Je0EDAReKvdNmZlZ9ZRzufG4iHhb0gHAZ4Hbgf9qbqOIWAGcBdwLPEd29dezki6SdFQq9i3gFEkzgd8AX4mI+tNss9LyO4DTI2LJ2nsxM7P1TTmDUK5Mf/8f8MuIuFPS98qpPCLuJruEOL/sB7np2cCBJba7E7iznH2Ymdn6pZzEUifpGuAzwD7ptFZZz3ExM7MNT7mPJv4rcEREvEE2dthad9GbmZlBGYklIt4BZgMjJJ0O9IyIeyoemZmZtUvlXBV2FjAR6JdeEyWdWenAzMysfSqnj+VUYHhquSDpEuBR4JeVDMzMzNqncvpYBHyYm/+Q0nfVm5mZldViuQV4XNKdZAnlaOCmikZlZuuFCU/MY9KMBS3aZnbdUgb16d58Qeuwyum8/ynZ6bBlwLtkNyv+rNKBmVn1TZqxgNl1S5svmDOoT3dGD92+QhFZe1BOiwWycbs+AFalv2a2gRjUpzu3n7Z/tcOwdqScq8IuIBtupQ/ZQJITJJ1f6cDMzKx9KqfF8mVgn4hYBiDpYmA68JNKBmZmZu1TOVeFvcqaCagzHmnYzMwaUU6LZRnwrKR7yR7UdRjwsKTLASLi3ArGZ2Zm7Uw5ieVP6VXv8QrFYmZmHUCziSUirm+LQMzMrGPw8PdmZlYoJxYzMytU2YlF0iaVDMTMzDqGcm6QHC7pGWBOmh8i6T8rHpmZmbVL5bRYriJ73v1igIiYCRxSyaDMzKz9KiexbBQRrzZYtrISwZiZWftXzn0s8yUNB0JSJ+Bs4IXKhmVmZu1VOS2WM4BzyR5L/BrwibTMzMxsLeXcIPk6cFwbxGJmZh1As4lF0nVkY4StISJOrUhEZmbWrpXTx3Jfbror8DlgfmXCMTOz9q6cU2G35+cl3QJMqVhEZmbWrq3LkC4DgB3LKSjpcEnPS5or6bwS6/tJelDS05JmSToit+78tN3zkkatQ5xmZlYF5fSxvMFHfSwbAUuAtZJEie06AdcAhwK1wDRJkyNidq7Y94CJEfErSYOAu4H+afo4YDCwHXCfpI9FhO+fMTNbzzWZWCQJGAIsSItWRcRaHfmNGA7MjYiXUl23AaOBfGIJoHua3hJYmKZHA7dFxAfAy5LmpvoeK3PfZpYz4Yl5TJqxoPmCDcyuW8qgPt2bL2iW0+SpsJREfh8RK9Or3KQCsD1rdvLXpmV544AvS6ola62c3YJtkXSqpBpJNYsWLWpBaGYblkkzFjC7bmmLtxvUpzujh671X8+sSeVcFfakpL0j4qkW1q0SyxompuOBGyPi55L2B26R9PEytyUixgPjAYYNG9aSpGe2wRnUpzu3n7Z/tcOwDUCjiUVS54hYARwEnCLpReBdsi/9iIi9m6m7FtghN9+Xj0511TsZOJyswsckdQV6lbmtmZmth5pqsTwJ7A0cvY51TwMGShpA1kdzHPClBmXmASOBGyXtTnafzCJgMjBB0uVknfcDUzxmZraeayqxCCAiXlyXiiNihaSzgHuBTsANEfGspIuAmoiYDHwLuE7SOWSnur6S+nGelTSRrKN/BfA1XxFmZtY+NJVYeks6t7GVEXF5c5VHxN1knfL5ZT/ITc8GDmxk24uBi5vbh1l7tq5Xa7WUr+6yttTUVWGdgC2Abo28zKyV1vVqrZby1V3WlppqsdRFxEVtFonZBspXa1lH01SLpdQlv2ZmZk1qKrGMbLMozMysw2g0sUTEkrYMxMzMOoZ1Gd3YzMysUU4sZmZWKCcWMzMrlBOLmZkVyonFzMwK5cRiZmaFKud5LGbtUluNw9UaHsPLOiK3WKzDaqtxuFrDY3hZR+QWi3VoHofLrO25xWJmZoVyYjEzs0I5sZiZWaGcWMzMrFBOLGZmVignFjMzK5QTi5mZFcqJxczMCuUbJK1dWJfhWTxcill1uMVi7cK6DM/i4VLMqsMtFms3PDyLWfvgFouZmRXKicXMzApV0cQi6aVRSzwAAAy9SURBVHBJz0uaK+m8EuuvkDQjvV6Q9GZu3crcusmVjNPMzIpTsT4WSZ2Aa4BDgVpgmqTJETG7vkxEnJMrfzawV66K9yJiaKXiMzOzyqhki2U4MDciXoqI5cBtwOgmyh8P/KaC8ZiZWRuoZGLZHpifm69Ny9YiaUdgAPBAbnFXSTWSHpd0dOXCNDOzIlXycmOVWBaNlD0OuCMiVuaW9YuIhZJ2Ah6Q9ExEvLjGDqRTgVMB+vXrV0TMZmbWSpVssdQCO+Tm+wILGyl7HA1Og0XEwvT3JWAqa/a/1JcZHxHDImJY7969i4jZzMxaqZKJZRowUNIASRuTJY+1ru6StCvQE3gst6ynpE3SdC/gQGB2w23NzGz9U7FTYRGxQtJZwL1AJ+CGiHhW0kVATUTUJ5njgdsiIn+abHfgvyStIkt+l+avJjMzs/VXRYd0iYi7gbsbLPtBg/lxJbZ7FNijkrGZmVll+M57MzMrlBOLmZkVyonFzMwK5cRiZmaFcmIxM7NCObGYmVmhnFjMzKxQTixmZlYoJxYzMyuUE4uZmRWqokO6WMc24Yl5TJqxoE32NbtuKYP6dG+TfZlZ67jFYuts0owFzK5b2ib7GtSnO6OHlnxOnJmtZ9xisVYZ1Kc7t5+2f7XDMLP1iFssZmZWKCcWMzMrlBOLmZkVyonFzMwK5cRiZmaFcmIxM7NCObGYmVmhnFjMzKxQTixmZlYoJxYzMyuUE4uZmRXKicXMzArlxGJmZoVyYjEzs0I5sZiZWaEqmlgkHS7peUlzJZ1XYv0Vkmak1wuS3sytO0nSnPQ6qZJxmplZcSr2oC9JnYBrgEOBWmCapMkRMbu+TESckyt/NrBXmt4KuBAYBgQwPW37RqXiNTOzYlSyxTIcmBsRL0XEcuA2YHQT5Y8HfpOmRwFTImJJSiZTgMMrGKuZmRWkko8m3h6Yn5uvBfYrVVDSjsAA4IEmtl3rgeeSTgVOTbMfSPp7K2NuC72Af1Q7iDKUHefE0yscSdM63PGsMsdZnPYQI8CuRVdYycSiEsuikbLHAXdExMqWbBsR44HxAJJqImLYugTalhxnsRxnsRxncdpDjJDFWXSdlTwVVgvskJvvCyxspOxxfHQarKXbmpnZeqSSiWUaMFDSAEkbkyWPyQ0LSdoV6Ak8llt8L3CYpJ6SegKHpWVmZraeq9ipsIhYIekssoTQCbghIp6VdBFQExH1SeZ44LaIiNy2SyT9iCw5AVwUEUua2eX4gt9CpTjOYjnOYjnO4rSHGKECcSr3fW5mZtZqvvPezMwK5cRiZmaFWm8TSxnDwWwi6fa0/glJ/XPrzk/Ln5c0qtw62zJOSYdKmi7pmfT3n3PbTE111g93s00V4+wv6b1cLNfmttknxT9X0lWSSl0m3hYx/ksuvhmSVkkamtZV41h+UtJTklZIOqbBupJDFRV9LFsTp6Shkh6T9KykWZLG5NbdKOnl3PEcWq0407qVuVgm55YPSJ+ROekzs3G14pR0SIPP5/uSjk7rqnE8z5U0O/3b3q/sPsL6dcV8PiNivXuRdfa/COwEbAzMBAY1KHMmcG2aPg64PU0PSuU3Ibvp8sVUX7N1tnGcewHbpemPAwty20wFhq0nx7M/8PdG6n0S2J/svqN7gM9UI8YGZfYAXqrysewP7AncDByTW74V8FL62zNN9yz6WBYQ58eAgWl6O6AO6JHmb8yXrebxTOveaaTeicBxafpa4IxqxtngM7AE2KyKx/OQ3P7P4KP/64V9PtfXFks5w8GMBm5K03cAI1MWHU12ldkHEfEyMDfV19IhZioaZ0Q8HRH19+Y8C3SVtEkr4yk8zsYqlNQH6B4Rj0X2ybsZOHo9iDE/NFAlNBtnRLwSEbOAVQ22LTlUUQWOZavijIgXImJOml4IvA70bmU8hcfZmPSZ+Geyzwhkn5mqHc8GjgHuiYhlrYynNXE+mNv/42T3CUKBn8/1NbGUM6TL6jIRsQJ4C9i6iW3LGiamDePM+wLwdER8kFv236lp/P0CTou0Ns4Bkp6W9FdJB+fK1zZTZ1vGWG8MayeWtj6WLd226GPZ1L5aRNJwsl++L+YWX5xOo1xRwI+h1sbZVVKNpMfrTy+RfSbeTJ+RdamzEnHWa3gzOFT3eJ5M1gJpatsWfz7X18RSzpAujZVp6fLWaE2c2UppMHAZcFpu/b9ExB7Awel1QhXjrAP6RcRewLnABEndy6yzrWLMVkr7AcsiIj9mXDWOZUu3rdZns+kKsl+qtwBfjYj6X+HnA7sB+5KdMvlOa4Kk9XH2i2zYlC8BV0rauYA6SynqeO7Bmjd7V+14Svoy2Qjy/9HMti1+7+trYilnSJfVZSR1BrYkO3fZ2LaVGCamNXEiqS/we+DEiFj9izAiFqS/bwMTyJq3VYkznVJcnOKZTvbL9WOpfN/c9q09nq06lslavwardCxbum3Rx7KpfZUl/Xj4E/C9iHi8fnlE1EXmA+C/qe7xrD9VR0S8RNafthfZwI890mekxXVWIs7ki8DvI+LD+gXVOp6SPg1cAByVO1NS3OezqE6jIl9kIwK8RNb5Xt8BNbhBma+xZkfuxDQ9mDU7718i69Bqts42jrNHKv+FEnX2StNdyM4Tn17FOHsDndL0TsACYKs0Pw34BB916B1RjRjT/EZk/wF2qvaxzJW9kbU7718m6xjtmaYLP5YFxLkxcD/wzRJl+6S/Aq4ELq1inD2BTdJ0L2AOqaMa+C1rdt6fWa04c8sfBw6p9vEkS74vki7QqMTnc53fQKVfwBHAC+kAXJCWXUSWYQG6pg/PXLIrFvJfKBek7Z4nd/VCqTqrFSfwPeBdYEbutQ2wOTAdmEXWqf8L0hd7leL8QopjJvAU8NlcncOAv6c6ryaN5FClf/MRwOMN6qvWsdyXLMm9CywGns1tOzbFP5fsFFNFjmVr4gS+DHzY4LM5NK17AHgmxfo/wBZVjPOAFMvM9PfkXJ07pc/I3PSZ2aTK/+79yX6UbdSgzmocz/uA13L/tpOL/nx6SBczMyvU+trHYmZm7ZQTi5mZFcqJxczMCuXEYmZmhXJiMTOzQjmxWLvTYETbGcqNbF2ibH9Jf29sfVuSNEzSVWl6hKQDcutOl3RiG8YyVNIRbbU/27BU7NHEZhX0XkS0enjxthYRNUBNmh0BvAM8mtZd28hm60xS5/hovKyGhpLdm3B30fs1c4vFOoTUMnkoPQ/jqXxrIFdmsKQnUytnlqSBafmXc8v/S1KnEtu+IumyVO5JSbuk5TumZ1rUP9uiX1p+rKS/S5op6W9p2QhJd6UW1unAOWmfB0saJ+nbknaX9GSD9zUrTe+TBgKdLuneNPZUwzhvlHS5pAeByyQNl/RoGkT0UUm7Kns2yUXAmLT/MZI2l3SDpGmpbGtH/rYNWWvv8vTLr7Z+ASv56K7h36dlmwFd0/RAoCZN9yc9Twb4T7JBKSEb7mJTYHfgj0CXtPyXZGO3NdznK3x0F/OJwF1p+o/ASWl6LPCHNP0MsH2arn+WyYjcduOAb+fqXz2f3lf9yAffIRuloQtZ66Z3Wj4GuKFEnDcCd/HRMDzdgc5p+tPAnWn6K8DVue0uAb5cHy/ZndubV/vf2q/2+fKpMGuPSp0K6wJcrewJfCvJBsps6DHggjT45+8iYo6kkcA+wLQ0ov6mZM8fKeU3ub9XpOn9gc+n6VuAn6bpR4AbJU0EfteSN0f2kKovApeSJZAxwK5kD4SbkuLsRDbydCm/jYiVaXpL4KbUOguy41TKYcBRkr6d5rsC/YDnWhi7mROLdRjnkI1/NITsFO/7DQtExARJTwBHAvdK+leyQfVuiojzy9hHNDK9VpmIOD0N438k0NJHzt4O/FbS77KqYo6kPcjGntq/jO3fzU3/CHgwIj6XTsFNbWQbkQ2I+nwL4jQryX0s1lFsCdRF9tyQE8h+0a9B0k5kjy2+CphM9hjZ+4FjJG2Tymyl3DPAGxiT+/tYmn6UbKRlgH8BHk717BwRT0TED8iGcc8PRw7wNtCt1E4ie4TCSuD7ZEkGsgFVe0vaP9XfJT3Lpzlbkg1+CNnpr8b2fy9wtlJzSNJeZdRtVpITi3UUvwROkvQ42Wmwd0uUGQP8XdIMsocr3RwRs8n6MP6SOsmnAGt1iiebpBbPN8haSABfB76atj0hrQP4D0nPpEud/0Y2Am/eH4HP1Xfel9jX7WSjDE8EiOwxs8eQdcjPJOuHWesChRJ+CvxE0iOsmWwfBAbVd96TtWy6ALNSzD8qo26zkjy6sVkZJL0CDIuIf1Q7FrP1nVssZmZWKLdYzMysUG6xmJlZoZxYzMysUE4sZmZWKCcWMzMrlBOLmZkV6v8DVuizaHxUlqsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fprs[0], tprs[0], label='AlexNet (area = {:.3f})'.format(aucs[0]))\n",
    "plt.plot(mlp_fprs[0], mlp_tprs[0], label='MLP (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.7, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fprs[0], tprs[0], label='AlexNet (area = {:.3f})'.format(aucs[0]))\n",
    "plt.plot(mlp_fprs[0], mlp_tprs[0], label='MLP (area = {:.3f})'.format(aucs[0]))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.02325581, 0.02325581, 0.04651163, 0.04651163,\n",
       "       0.04651163, 0.06976744, 0.06976744, 0.09302326, 0.09302326,\n",
       "       0.11627907, 0.11627907, 0.13953488, 0.13953488, 0.18604651,\n",
       "       0.18604651, 0.25581395, 0.27906977, 0.30232558, 0.30232558,\n",
       "       0.3255814 , 0.3255814 , 0.34883721, 0.34883721, 0.37209302,\n",
       "       0.37209302, 0.60465116, 0.60465116, 0.62790698, 0.62790698,\n",
       "       0.65116279, 0.65116279, 0.6744186 , 0.6744186 , 0.69767442,\n",
       "       0.69767442, 0.72093023, 0.72093023, 0.74418605, 0.74418605,\n",
       "       0.76744186, 0.76744186, 0.88372093, 0.88372093, 0.93023256,\n",
       "       0.93023256, 1.        ])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test_ohe[:,0], y_pred_keras[:,0])\n",
    "tpr_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "#print(aucs, fprs, tprs)\n",
    "print(len(fprs[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.callbacks.callbacks.History at 0x26da3a8d688>,\n",
       " <keras.callbacks.callbacks.History at 0x26db2726d48>,\n",
       " <keras.callbacks.callbacks.History at 0x26db67cbe48>,\n",
       " <keras.callbacks.callbacks.History at 0x26db76ad848>,\n",
       " <keras.callbacks.callbacks.History at 0x26dbb630548>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_histories[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.3086502356529236,\n",
       "  0.3283050652742386,\n",
       "  0.292212664604187,\n",
       "  0.2826671036481857,\n",
       "  0.23993148624897004,\n",
       "  0.2301979398727417,\n",
       "  0.2715948328971863,\n",
       "  0.4862943035364151],\n",
       " 'val_recall_6': [0.9039999842643738,\n",
       "  0.8880000114440918,\n",
       "  0.9200000166893005,\n",
       "  0.871999979019165,\n",
       "  0.8880000114440918,\n",
       "  0.9039999842643738,\n",
       "  0.8960000276565552,\n",
       "  0.8640000224113464],\n",
       " 'loss': [0.268541384109937,\n",
       "  0.20544711815774755,\n",
       "  0.19613678438893906,\n",
       "  0.18069041215864842,\n",
       "  0.17609683662239048,\n",
       "  0.15704124928178156,\n",
       "  0.14645729406746646,\n",
       "  0.11541567897236543],\n",
       " 'recall_6': [0.8964497,\n",
       "  0.92228836,\n",
       "  0.9276792,\n",
       "  0.93180305,\n",
       "  0.9381614,\n",
       "  0.94183004,\n",
       "  0.9490741,\n",
       "  0.9566799]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_histories[0].history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = old_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.callbacks.callbacks.History at 0x26da3a8d688>,\n",
       " <keras.callbacks.callbacks.History at 0x26db2726d48>,\n",
       " <keras.callbacks.callbacks.History at 0x26db67cbe48>,\n",
       " <keras.callbacks.callbacks.History at 0x26db76ad848>,\n",
       " <keras.callbacks.callbacks.History at 0x26dbb630548>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alexnet.txt\", \"w\") as f:\n",
    "    for history in test:\n",
    "        print(history.history, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "histories = []\n",
    "max_length = 0\n",
    "with open(\"alexnet.txt\", \"r\") as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        histories.append(ast.literal_eval(line))\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.3086502356529236,\n",
       "  0.3283050652742386,\n",
       "  0.292212664604187,\n",
       "  0.2826671036481857,\n",
       "  0.23993148624897004,\n",
       "  0.2301979398727417,\n",
       "  0.2715948328971863,\n",
       "  0.4862943035364151],\n",
       " 'val_recall_6': [0.9039999842643738,\n",
       "  0.8880000114440918,\n",
       "  0.9200000166893005,\n",
       "  0.871999979019165,\n",
       "  0.8880000114440918,\n",
       "  0.9039999842643738,\n",
       "  0.8960000276565552,\n",
       "  0.8640000224113464],\n",
       " 'loss': [0.268541384109937,\n",
       "  0.20544711815774755,\n",
       "  0.19613678438893906,\n",
       "  0.18069041215864842,\n",
       "  0.17609683662239048,\n",
       "  0.15704124928178156,\n",
       "  0.14645729406746646,\n",
       "  0.11541567897236543],\n",
       " 'recall_6': [0.8964497,\n",
       "  0.92228836,\n",
       "  0.9276792,\n",
       "  0.93180305,\n",
       "  0.9381614,\n",
       "  0.94183004,\n",
       "  0.9490741,\n",
       "  0.9566799]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets get the average for each fold\n",
    "#lets save the longest length item\n",
    "import numpy as np\n",
    "recalls = np.array([list(v for k,v in history.items() if k.startswith('recall')) for history in histories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for recall in recalls:\n",
    "    tmp.append(recall[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[[1,2],[1,2,3],[1]]\n",
    "length = max(map(len, tmp))\n",
    "recalls=np.array([xi+[None]*(length-len(xi)) for xi in tmp])\n",
    "recalls = np.array(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "for row in range(len(recalls[0])):\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    for item in recalls[:,row]:\n",
    "        if item != None:\n",
    "            count += 1\n",
    "            sum += item\n",
    "    means.append(sum/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.93794578,\n",
       " 0.9468464320000001,\n",
       " 0.955171754,\n",
       " 0.9486528333333334,\n",
       " 0.9474765666666666,\n",
       " 0.950908455,\n",
       " 0.9490741,\n",
       " 0.9566799]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
