{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is the lab...\n",
    "Logistic regression and stuff\n",
    "so yeah... we need paramters for regularization\n",
    "none, L1, L2, this can jsut be a flag with hyperparamters and overloaded functions for the regulairzation...\n",
    "well maybe not overloading but a method that we call that calls other methods called L1 and L2\n",
    "we can compare different regularizations within the same method, that way its legit. \n",
    "for the methods... Were suggetsted to the Stochiastic graident descent. mini-batch gradient descent for our steepest descent and the hessian for the quasi newton thingy. Frick do we gotta write our own hessian? Thatll suck big time\n",
    "\n",
    "Step1.\n",
    "Read data and encode locations as one hot encoded\n",
    "Step2.\n",
    "Split into 80% train and 20 %test after merging.\n",
    "Step3. Write regularization\n",
    "Do these tonight\n",
    "\n",
    "\n",
    "We did both of the regularzations but not together \\for reg we do ridge lasso or elastic net \n",
    "gradient[1:] = regularization + get gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Three: Extending Logistic Regression\n",
    "by:\n",
    "* Hayden Donofrio\n",
    "* Riley Bates\n",
    "* Chandler Choate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Preparation and Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to shop for clothing items online, it can be difficult to determine the correct sizing for your shape. Most website just have your basic options of small, medium, large, etc., but how do you know which option would work best for you? Life would be so much easier if you knew the item you ordered would come in fitting exactly the way you imagined it would. From using the content in this dataset, we can write an algorithm that when given specific measurements, it will map to previous successful similar entries and return the best fit for your features. The ability to achieve this approximate \"fit\", allows the customer to feel more confident in their sizing selection process.\n",
    "\n",
    "According to https://www.shopify.com/enterprise/ecommerce-returns, seventy-two percent of all of their customer returns are due to issues with the fit, sizing, style, etc. When researching different companies on their return rates, the customer-preference based returns are constantly receiving the highest percentages.\n",
    "\n",
    "With a successful classification algorithm, this could be deployed into hundreds of online shopping stores and provide a better experience throughout. For example, people would not need to go through the hassle of repackaging and shipping their item, the money they are having to spend for this process, working with customer service on an exchange or refund, etc. On the business side, companies will be able to help more customers due to the decrease of services needed, receive higher ratings from customer satisfaction, and much more. By having this algorithm in place, the world of online shopping would increase, and it would be easier to obtain the products you need, when you do not have time in your schedule, and have no doubts about the fit of your product.\n",
    "\n",
    "Dataset Source: https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation/data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothing-fit-dataset-for-size-recommendation\n",
      "NaNs per column, if more than 20% we will drop otherwise\n",
      "6018 bra size\n",
      "70936 bust\n",
      "6255 cup size\n",
      "1107 height\n",
      "26726 hips\n",
      "35 length\n",
      "68 quality\n",
      "54875 shoe size\n",
      "0 size\n",
      "79908 waist\n",
      "    bra size  cup size     height       hips  length  quality  shoe size  size\n",
      "0  34.000000       4.0  77.000000  38.000000     3.0      5.0   8.145818     7\n",
      "1  36.000000       2.0  29.000000  30.000000     3.0      3.0   8.145818    13\n",
      "2  32.000000       2.0  89.000000  40.358501     4.0      2.0   9.000000     7\n",
      "3  35.972125       5.0  67.016136  40.358501     3.0      5.0   8.145818    21\n",
      "4  36.000000       2.0  29.000000  40.358501     4.0      5.0   8.145818    18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[34,  4, 77, ...,  5,  8,  7],\n",
       "       [36,  2, 29, ...,  3,  8, 13],\n",
       "       [32,  2, 89, ...,  2,  9,  7],\n",
       "       ...,\n",
       "       [32,  7, 53, ...,  5,  8, 12],\n",
       "       [35,  3, 41, ...,  4,  8, 12],\n",
       "       [32,  4,  6, ...,  4,  8,  4]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the block that simply reads the data\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "def translate_height(height):\n",
    "    height = str(height)\n",
    "    if height == \"nan\":\n",
    "        return None\n",
    "    l = re.findall(r'[0-9]+',height)\n",
    "    if len(l) == 1:\n",
    "        return int(l[0])\n",
    "    else:\n",
    "        return int(l[0]) + int(l[1])*12\n",
    "directory = os.path.relpath(\"clothing-fit-dataset-for-size-recommendation/\")\n",
    "print(directory)\n",
    "df = []\n",
    "y = []\n",
    "df = pd.read_json('clothing-fit-dataset-for-size-recommendation/modcloth_final_data.json', lines=True)\n",
    "di = {\"very short\": 1, \"slightly short\": 2, \"just right\": 3, \"slightly long\": 4, \"very long\": 5}\n",
    "df = df.replace({\"length\": di})\n",
    "di = {\"small\": 1, \"fit\": 2, \"large\": 3}\n",
    "df = df.replace({\"fit\": di})\n",
    "df[\"height\"] = df[\"height\"].map(lambda x: translate_height(x))\n",
    "\n",
    "di = {\"aa\": 0, \"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"dd/e\": 5, \"ddd/f\": 6, \"dddd/g\": 7, \"h\": 8, \"i\": 9, \"j\": 10, \"k\": 11}\n",
    "df = df.replace({\"cup size\": di})\n",
    "#maybe add shoe width back?\n",
    "y = df['fit']\n",
    "df = df.drop(columns=[\"item_id\", \"category\", \"user_name\", \"user_id\", \"shoe width\", \"review_summary\", \"review_text\", \"fit\"])\n",
    "print(\"NaNs per column, if more than 20% we will drop otherwise\")\n",
    "for column in df:\n",
    "    print(df[column].isna().sum(), column)\n",
    "    df[column] = pd.to_numeric(df[column], errors=\"coerce\")\n",
    "df = df.drop(columns=[\"waist\", \"bust\"])\n",
    "df = df.fillna(df.mean())\n",
    "print(df.head())\n",
    "#big issue here is predicting columns with shoes? wtf there are so many NaNs. Not sure what to do about shoe products?\n",
    "x = df.values.astype(int)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        2\n",
       "4        1\n",
       "5        1\n",
       "6        3\n",
       "7        1\n",
       "8        1\n",
       "9        2\n",
       "10       1\n",
       "11       1\n",
       "12       2\n",
       "13       2\n",
       "14       2\n",
       "15       2\n",
       "16       2\n",
       "17       3\n",
       "18       3\n",
       "19       2\n",
       "20       3\n",
       "21       3\n",
       "22       1\n",
       "23       1\n",
       "24       2\n",
       "25       1\n",
       "26       2\n",
       "27       1\n",
       "28       2\n",
       "29       1\n",
       "        ..\n",
       "82760    2\n",
       "82761    2\n",
       "82762    1\n",
       "82763    2\n",
       "82764    2\n",
       "82765    2\n",
       "82766    2\n",
       "82767    1\n",
       "82768    2\n",
       "82769    2\n",
       "82770    1\n",
       "82771    2\n",
       "82772    2\n",
       "82773    2\n",
       "82774    2\n",
       "82775    2\n",
       "82776    2\n",
       "82777    1\n",
       "82778    1\n",
       "82779    1\n",
       "82780    2\n",
       "82781    1\n",
       "82782    2\n",
       "82783    2\n",
       "82784    3\n",
       "82785    2\n",
       "82786    1\n",
       "82787    2\n",
       "82788    2\n",
       "82789    2\n",
       "Name: fit, Length: 82790, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is for the renttherunway stuff\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61365    3\n",
       "25980    2\n",
       "25419    2\n",
       "81249    2\n",
       "33715    2\n",
       "52862    3\n",
       "41124    2\n",
       "358      2\n",
       "38390    2\n",
       "50399    2\n",
       "76900    2\n",
       "8845     2\n",
       "9270     2\n",
       "56637    2\n",
       "20025    2\n",
       "27428    2\n",
       "20541    2\n",
       "32717    2\n",
       "2364     2\n",
       "23528    2\n",
       "24128    2\n",
       "11848    2\n",
       "34232    1\n",
       "8948     2\n",
       "51720    2\n",
       "33309    2\n",
       "81484    3\n",
       "40278    2\n",
       "65215    2\n",
       "217      3\n",
       "        ..\n",
       "29534    2\n",
       "23860    2\n",
       "3834     3\n",
       "72873    2\n",
       "9618     1\n",
       "55595    2\n",
       "20335    1\n",
       "11861    2\n",
       "8669     2\n",
       "72425    2\n",
       "4968     2\n",
       "53740    2\n",
       "22545    2\n",
       "2875     2\n",
       "10342    2\n",
       "46747    1\n",
       "60026    2\n",
       "48462    2\n",
       "14507    2\n",
       "62664    2\n",
       "11265    1\n",
       "4406     1\n",
       "64857    3\n",
       "66426    2\n",
       "43895    2\n",
       "49149    2\n",
       "74570    2\n",
       "38824    2\n",
       "81584    2\n",
       "24808    2\n",
       "Name: fit, Length: 16558, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now split into training and testing data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = .80)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.6886248339171398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#test logistic regresion to see if my splits are working\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = LogisticRegression(fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "yhat = clf.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # this typically gives us the probability of a specific class\n",
    "        # we need a bias term \n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        # this will take the x with the bias term and multiplies it by w. This will give us a scalr quantity\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        # if we actually want to do a prediction we are seeing if it is greater than point 5. vector of ones were it is greater than .5 and 0 when it is below .5\n",
    "        # When one is greater than the other it would be greater than .5\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        #for each vector x we want to out it throught he sigmoid suntract it from y and add it to the gradient function\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi \n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        #return gradient divided by the total number of points\n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        #this is going to get features and labels. We are going to say x wtih a bias is ther (col of 1s)\n",
    "        #the shape is the # of samples (rows ) by # of features (cols)\n",
    "        #w always starts at 0 usually. \n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        # whats the gradient?? and internall we are updating self.w_ in this\n",
    "        \n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            #put regularization here\n",
    "            regularization = \"L1\"\n",
    "            \n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval) # create a binary problem whenever class is 0 (big class), y binary will be 1\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X),axis=1)] # take argmax along row\n",
    "    \n",
    "#print(X)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yhat = lr.predict(X_test)\n",
    "#print('Accuracy of: ',accuracy_score(y_test,yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    # extend init functions\n",
    "    def __init__(self, C=0.0,reg=\"BOTH\", **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        self.reg = reg\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "        \n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self,X,y):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X,y)\n",
    "        \n",
    "        # add in regularization (to all except bias term)\n",
    "        #this is L1\n",
    "        div = np.divide(np.absolute(self.w_[1:]).T[0], self.w_[1:,0], out=np.zeros_like(np.absolute(self.w_[1:]).T[0]), where=self.w_[1:,0]!=0)\n",
    "        div = div.T.reshape(self.w_[1:].shape)\n",
    "        if self.reg == \"L1\" or self.reg == \"BOTH\":\n",
    "            gradient[1:] += -1 * div  * self.C\n",
    "        if self.reg == \"BOTH\" or self.reg == \"L2\":\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now redefine the Logistic Regression Function where needed\n",
    "class RegularizedLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, C=0.0,reg=\"BOTH\", **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        self.reg = reg\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                      C=self.C,\n",
    "                                                     reg=self.reg)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            print(blr)\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-6.63226237]\n",
      " [-1.34859134]\n",
      " [-0.10933566]\n",
      " [-2.41954419]\n",
      " [-1.51799114]\n",
      " [-0.1639332 ]\n",
      " [-0.24682487]\n",
      " [-0.31424279]\n",
      " [-0.28692305]]\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 5.59302452]\n",
      " [ 0.82587521]\n",
      " [ 0.03334983]\n",
      " [ 1.53078998]\n",
      " [ 0.89741121]\n",
      " [ 0.05811809]\n",
      " [ 0.21760797]\n",
      " [ 0.20205911]\n",
      " [-0.19260979]]\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-6.59268033]\n",
      " [ 0.140147  ]\n",
      " [ 0.03685648]\n",
      " [ 0.1352954 ]\n",
      " [ 0.19205814]\n",
      " [ 0.07336647]\n",
      " [-0.01226453]\n",
      " [ 0.02775056]\n",
      " [ 0.32981454]]\n",
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-6.63226237 -1.34859134 -0.10933566 -2.41954419 -1.51799114 -0.1639332\n",
      "  -0.24682487 -0.31424279 -0.28692305]\n",
      " [ 5.59302452  0.82587521  0.03334983  1.53078998  0.89741121  0.05811809\n",
      "   0.21760797  0.20205911 -0.19260979]\n",
      " [-6.59268033  0.140147    0.03685648  0.1352954   0.19205814  0.07336647\n",
      "  -0.01226453  0.02775056  0.32981454]]\n"
     ]
    }
   ],
   "source": [
    "lr = RegularizedLogisticRegression(eta=0.1,\n",
    "                                           iterations=2500,\n",
    "                                           C=.4, \n",
    "                                  reg = \"L2\") # get object(0.1,500)\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.6859524097113178\n"
     ]
    }
   ],
   "source": [
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minibatch s essentially just running regression a smaller batch. This will be much faster with hopefully similar accuracy.\n",
    "from numpy import random\n",
    "class StochasticGradientDescent(RegularizedLogisticRegression):\n",
    "    def __init__(self,size=1000,reg=\"BOTH\", **kwds):\n",
    "        self.reg = reg\n",
    "        self.size = size\n",
    "            #make sure to pass c, eta and iterations\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        #get a random sample \n",
    "        x_slice = X[self.size:self.size*2,:]\n",
    "        y_slice = y[self.size:self.size*2]\n",
    "        print(y_slice)\n",
    "        self.unique_ = np.unique(y_slice) # get each unique class value\n",
    "        \n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        #then we will\n",
    "        print(np.unique(y_slice))\n",
    "        #there are nans in here\n",
    "        print(self.unique_)\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            print(i)\n",
    "            print(yval)\n",
    "            y_binary = y_slice==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                          C=self.C,\n",
    "                                                     reg=self.reg)\n",
    "            blr.fit(x_slice,y_binary)\n",
    "                # add the trained classifier to the list\n",
    "            print(blr)\n",
    "            self.classifiers_.append(blr)\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        #stochastic takes a single sample \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60756    2\n",
      "16637    2\n",
      "6064     1\n",
      "1352     3\n",
      "36254    1\n",
      "47280    2\n",
      "48066    2\n",
      "23793    2\n",
      "287      2\n",
      "63521    2\n",
      "28062    2\n",
      "8835     2\n",
      "54014    2\n",
      "62078    2\n",
      "440      2\n",
      "63684    2\n",
      "11593    2\n",
      "16701    2\n",
      "32647    2\n",
      "66811    2\n",
      "12824    1\n",
      "58300    2\n",
      "42549    3\n",
      "49733    3\n",
      "56646    3\n",
      "65667    2\n",
      "56638    3\n",
      "73747    1\n",
      "58344    2\n",
      "25765    2\n",
      "        ..\n",
      "69799    2\n",
      "58788    2\n",
      "77342    2\n",
      "25913    2\n",
      "75776    2\n",
      "22798    2\n",
      "35175    2\n",
      "437      1\n",
      "49781    2\n",
      "28572    2\n",
      "61417    3\n",
      "39276    2\n",
      "31747    1\n",
      "81633    2\n",
      "47982    2\n",
      "6090     1\n",
      "14424    2\n",
      "69961    2\n",
      "1177     1\n",
      "58824    3\n",
      "21858    2\n",
      "73144    3\n",
      "70225    1\n",
      "45496    2\n",
      "37703    2\n",
      "29421    3\n",
      "31267    3\n",
      "47031    2\n",
      "72898    2\n",
      "14595    2\n",
      "Name: fit, Length: 1000, dtype: int64\n",
      "[1 2 3]\n",
      "[1 2 3]\n",
      "0\n",
      "1\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-8.98371029e+00]\n",
      " [-5.55469472e-01]\n",
      " [ 6.24262142e-02]\n",
      " [-1.06930726e+00]\n",
      " [-5.63820786e-01]\n",
      " [ 3.93391783e-02]\n",
      " [ 1.29418135e-02]\n",
      " [-3.03870437e-03]\n",
      " [ 3.58680541e-02]]\n",
      "1\n",
      "2\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 8.09999737]\n",
      " [ 1.10244398]\n",
      " [-0.0199468 ]\n",
      " [ 1.40448401]\n",
      " [ 1.14539562]\n",
      " [ 0.0500883 ]\n",
      " [ 0.11498875]\n",
      " [ 0.11821498]\n",
      " [-0.14527494]]\n",
      "2\n",
      "3\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-8.20740031]\n",
      " [ 0.20817189]\n",
      " [ 0.07886363]\n",
      " [ 0.2502844 ]\n",
      " [ 0.18804026]\n",
      " [ 0.09526805]\n",
      " [ 0.02568502]\n",
      " [ 0.10685703]\n",
      " [ 0.43187226]]\n"
     ]
    }
   ],
   "source": [
    "#print(y_train)\n",
    "#5194\n",
    "ls = StochasticGradientDescent(eta=0.1,\n",
    "                                           iterations=2500, size=1000,\n",
    "                                           C=.4,\n",
    "                              reg=\"L2\") # get object(0.1,500)\n",
    "\n",
    "#print( len(y_train))\n",
    "\n",
    "ls.fit(X_train,y_train)\n",
    "#print(ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.6859524097113178\n"
     ]
    }
   ],
   "source": [
    "yhat = ls.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Newton(RegularizedLogisticRegression):\n",
    "    def sig(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    def __init__(self,size=1000, **kwds):\n",
    "        self.size = size\n",
    "            #make sure to pass c, eta and iterations\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self,X,y):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X,y)\n",
    "        \n",
    "        S = diag(self.sig(X @ self.w_)*(1-self.sig(X @ self.w_)))\n",
    "        hessian = (X.T*S*X)\n",
    "        H_inv = np.linalg.inv(hessian)\n",
    "#         g = (X.T((self.sig(X @ self.w_)) - y))\n",
    "        newt_update = H_inv * gradient\n",
    "        gradient = newt_update\n",
    "        return gradient\n",
    "        \n",
    "    def fit(self, X, y):   \n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                      C=self.C)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            print(blr)\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-8.52131253]\n",
      " [ 0.47381672]\n",
      " [ 0.07952165]\n",
      " [ 0.96901111]\n",
      " [ 0.57214482]\n",
      " [ 0.00893662]\n",
      " [ 0.03887422]\n",
      " [ 0.14883157]\n",
      " [ 0.28359675]]\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 5.59302452]\n",
      " [ 0.65843738]\n",
      " [ 0.03792486]\n",
      " [ 1.36335215]\n",
      " [ 0.72997339]\n",
      " [ 0.04308199]\n",
      " [ 0.05017015]\n",
      " [ 0.03462129]\n",
      " [-0.05409042]]\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-8.22899845]\n",
      " [-1.78327451]\n",
      " [-0.14664394]\n",
      " [-3.44690123]\n",
      " [-1.98072945]\n",
      " [-0.09347265]\n",
      " [-0.20829746]\n",
      " [-0.34815698]\n",
      " [-0.40872242]]\n",
      "Accuracy of:  0.16265551395096026\n"
     ]
    }
   ],
   "source": [
    "ls = Newton(eta=0.1,iterations=2500, \n",
    "                   size=1000, C=0.4, reg=\"L2\")\n",
    "ls.fit(X_train,y_train)\n",
    "yhat = ls.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweaking logistic regression model for good generalization performance by adjusting the optimization method and the value of regularization term C. We will build a grid by trying all three optimization methods with four different values of c, and take the best accuracy as our optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "c_vals = [.00001, .001, .1, 1]\n",
    "accuracy_matrix = np.zeros((3,4))\n",
    "\n",
    "# Steepest Gradient Descent w/ all c vals\n",
    "for i in range(len(c_vals)):\n",
    "    ls = RegularizedLogisticRegression(eta=0.1,iterations=2500, \n",
    "                   size=1000, C=c_vals[i], reg=\"L2\")\n",
    "    ls.fit(X_train,y_train)\n",
    "    yhat = ls.predict(X_test)\n",
    "    accuracy_matrix[0,i] = accuracy_score(y_test,yhat)\n",
    "    \n",
    "# Stochastic Gradient Descent w/ all c vals\n",
    "for i in range(len(c_vals)):\n",
    "    ls = Newton(eta=0.1,iterations=2500, \n",
    "                   size=1000, C=c_vals[i], reg=\"L2\")\n",
    "    ls.fit(X_train,y_train)\n",
    "    yhat = ls.predict(X_test)\n",
    "    accuracy_matrix[1,i] = accuracy_score(y_test,yhat)\n",
    "    \n",
    "# Newton's Method w/ all c vals\n",
    "for i in range(len(c_vals)):\n",
    "    ls = Newton(eta=0.1,iterations=2500, \n",
    "                   size=1000, C=c_vals[i],  reg=\"L2\")\n",
    "    ls.fit(X_train,y_train)\n",
    "    yhat = ls.predict(X_test)\n",
    "    accuracy_matrix[2,i] = accuracy_score(y_test,yhat)\n",
    "    \n",
    "print(accuracy_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Exceptional Work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
