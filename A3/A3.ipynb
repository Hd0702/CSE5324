{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is the lab...\n",
    "Logistic regression and stuff\n",
    "so yeah... we need paramters for regularization\n",
    "none, L1, L2, this can jsut be a flag with hyperparamters and overloaded functions for the regulairzation...\n",
    "well maybe not overloading but a method that we call that calls other methods called L1 and L2\n",
    "we can compare different regularizations within the same method, that way its legit. \n",
    "for the methods... Were suggetsted to the Stochiastic graident descent. mini-batch gradient descent for our steepest descent and the hessian for the quasi newton thingy. Frick do we gotta write our own hessian? Thatll suck big time\n",
    "\n",
    "Step1.\n",
    "Read data and encode locations as one hot encoded\n",
    "Step2.\n",
    "Split into 80% train and 20 %test after merging.\n",
    "Step3. Write regularization\n",
    "Do these tonight\n",
    "\n",
    "\n",
    "We did both of the regularzations but not together \\for reg we do ridge lasso or elastic net \n",
    "gradient[1:] = regularization + get gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothing-fit-dataset-for-size-recommendation\n",
      "NaNs per column, if more than 20% we will drop otherwise\n",
      "79908 waist\n",
      "0 size\n",
      "68 quality\n",
      "6255 cup size\n",
      "26726 hips\n",
      "6018 bra size\n",
      "70936 bust\n",
      "1107 height\n",
      "35 length\n",
      "54875 shoe size\n",
      "   size  quality  cup size       hips   bra size     height  length  shoe size\n",
      "0     7      5.0       4.0  38.000000  34.000000  77.000000     3.0   8.145818\n",
      "1    13      3.0       2.0  30.000000  36.000000  29.000000     3.0   8.145818\n",
      "2     7      2.0       2.0  40.358501  32.000000  89.000000     4.0   9.000000\n",
      "3    21      5.0       5.0  40.358501  35.972125  67.016136     3.0   8.145818\n",
      "4    18      5.0       2.0  40.358501  36.000000  29.000000     4.0   8.145818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7,  5,  4, ..., 77,  3,  8],\n",
       "       [13,  3,  2, ..., 29,  3,  8],\n",
       "       [ 7,  2,  2, ..., 89,  4,  9],\n",
       "       ...,\n",
       "       [12,  5,  7, ..., 53,  3,  8],\n",
       "       [12,  4,  3, ..., 41,  3,  8],\n",
       "       [ 4,  4,  4, ...,  6,  3,  8]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the block that simply reads the data\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "def translate_height(height):\n",
    "    height = str(height)\n",
    "    if height == \"nan\":\n",
    "        return None\n",
    "    l = re.findall(r'[0-9]+',height)\n",
    "    if len(l) == 1:\n",
    "        return int(l[0])\n",
    "    else:\n",
    "        return int(l[0]) + int(l[1])*12\n",
    "directory = os.path.relpath(\"clothing-fit-dataset-for-size-recommendation/\")\n",
    "print(directory)\n",
    "df = []\n",
    "y = []\n",
    "df = pd.read_json('clothing-fit-dataset-for-size-recommendation/modcloth_final_data.json', lines=True)\n",
    "di = {\"very short\": 1, \"slightly short\": 2, \"just right\": 3, \"slightly long\": 4, \"very long\": 5}\n",
    "df = df.replace({\"length\": di})\n",
    "di = {\"small\": 1, \"fit\": 2, \"large\": 3}\n",
    "df = df.replace({\"fit\": di})\n",
    "df[\"height\"] = df[\"height\"].map(lambda x: translate_height(x))\n",
    "\n",
    "di = {\"aa\": 0, \"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"dd/e\": 5, \"ddd/f\": 6, \"dddd/g\": 7, \"h\": 8, \"i\": 9, \"j\": 10, \"k\": 11}\n",
    "df = df.replace({\"cup size\": di})\n",
    "#maybe add shoe width back?\n",
    "y = df['fit']\n",
    "df = df.drop(columns=[\"item_id\", \"category\", \"user_name\", \"user_id\", \"shoe width\", \"review_summary\", \"review_text\", \"fit\"])\n",
    "print(\"NaNs per column, if more than 20% we will drop otherwise\")\n",
    "for column in df:\n",
    "    print(df[column].isna().sum(), column)\n",
    "    df[column] = pd.to_numeric(df[column], errors=\"coerce\")\n",
    "df = df.drop(columns=[\"waist\", \"bust\"])\n",
    "df = df.fillna(df.mean())\n",
    "print(df.head())\n",
    "#big issue here is predicting columns with shoes? wtf there are so many NaNs. Not sure what to do about shoe products?\n",
    "x = df.values.astype(int)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        2\n",
       "4        1\n",
       "        ..\n",
       "82785    2\n",
       "82786    1\n",
       "82787    2\n",
       "82788    2\n",
       "82789    2\n",
       "Name: fit, Length: 82790, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is for the renttherunway stuff\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46339    2\n",
       "46904    2\n",
       "82525    2\n",
       "8667     2\n",
       "65509    2\n",
       "        ..\n",
       "6174     3\n",
       "16004    2\n",
       "31445    1\n",
       "73549    1\n",
       "25738    2\n",
       "Name: fit, Length: 16558, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now split into training and testing data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = .80)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydendonofrio/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/haydendonofrio/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.6896515279623143\n"
     ]
    }
   ],
   "source": [
    "#test logistic regresion to see if my splits are working\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = LogisticRegression(fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "yhat = clf.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regurlzarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # this typically gives us the probability of a specific class\n",
    "        # we need a bias term \n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        # this will take the x with the bias term and multiplies it by w. This will give us a scalr quantity\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        # if we actually want to do a prediction we are seeing if it is greater than point 5. vector of ones were it is greater than .5 and 0 when it is below .5\n",
    "        # When one is greater than the other it would be greater than .5\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        #for each vector x we want to out it throught he sigmoid suntract it from y and add it to the gradient function\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi \n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        #return gradient divided by the total number of points\n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        #this is going to get features and labels. We are going to say x wtih a bias is ther (col of 1s)\n",
    "        #the shape is the # of samples (rows ) by # of features (cols)\n",
    "        #w always starts at 0 usually. \n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        # whats the gradient?? and internall we are updating self.w_ in this\n",
    "        \n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval) # create a binary problem whenever class is 0 (big class), y binary will be 1\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X),axis=1)] # take argmax along row\n",
    "    \n",
    "#print(X)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yhat = lr.predict(X_test)\n",
    "#print('Accuracy of: ',accuracy_score(y_test,yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    # extend init functions\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "        \n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self,X,y):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X,y)\n",
    "        \n",
    "        # add in regularization (to all except bias term)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now redefine the Logistic Regression Function where needed\n",
    "class RegularizedLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, C=0.0, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                      C=self.C)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            print(blr)\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-7.21034139]\n",
      " [-0.58215164]\n",
      " [-0.31597234]\n",
      " [-0.18570778]\n",
      " [-2.21767644]\n",
      " [-1.9833122 ]\n",
      " [-3.9797578 ]\n",
      " [-0.21493154]\n",
      " [-0.44603108]]\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 4.46064138]\n",
      " [-0.21729823]\n",
      " [ 0.19244134]\n",
      " [ 0.02959124]\n",
      " [ 0.67154472]\n",
      " [ 0.63326681]\n",
      " [ 1.03332321]\n",
      " [ 0.03961603]\n",
      " [ 0.15079484]]\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-6.82342737e+00]\n",
      " [ 2.40631566e-01]\n",
      " [-5.93273242e-03]\n",
      " [ 2.37134647e-02]\n",
      " [ 1.54082619e-01]\n",
      " [ 1.18309483e-01]\n",
      " [ 2.24610276e-01]\n",
      " [ 7.38874688e-02]\n",
      " [ 2.40400657e-02]]\n",
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-7.21034139e+00 -5.82151643e-01 -3.15972343e-01 -1.85707779e-01\n",
      "  -2.21767644e+00 -1.98331220e+00 -3.97975780e+00 -2.14931543e-01\n",
      "  -4.46031076e-01]\n",
      " [ 4.46064138e+00 -2.17298233e-01  1.92441343e-01  2.95912432e-02\n",
      "   6.71544724e-01  6.33266812e-01  1.03332321e+00  3.96160322e-02\n",
      "   1.50794836e-01]\n",
      " [-6.82342737e+00  2.40631566e-01 -5.93273242e-03  2.37134647e-02\n",
      "   1.54082619e-01  1.18309483e-01  2.24610276e-01  7.38874688e-02\n",
      "   2.40400657e-02]]\n"
     ]
    }
   ],
   "source": [
    "lr = RegularizedLogisticRegression(eta=0.1,\n",
    "                                           iterations=2500,\n",
    "                                           C=.4) # get object(0.1,500)\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.6858316221765913\n"
     ]
    }
   ],
   "source": [
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minibatch s essentially just running regression a smaller batch. This will be much faster with hopefully similar accuracy.\n",
    "from numpy import random\n",
    "class StochasticGradientDescent(RegularizedLogisticRegression):\n",
    "    def __init__(self,size=1000, **kwds):\n",
    "        self.size = size\n",
    "            #make sure to pass c, eta and iterations\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        #get a random sample \n",
    "        x_slice = X[self.size:self.size*2,:]\n",
    "        y_slice = y[self.size:self.size*2]\n",
    "        print(y_slice)\n",
    "        self.unique_ = np.unique(y_slice) # get each unique class value\n",
    "        \n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        #then we will\n",
    "        print(np.unique(y_slice))\n",
    "        #there are nans in here\n",
    "        print(self.unique_)\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            print(i)\n",
    "            print(yval)\n",
    "            y_binary = y_slice==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                          C=self.C)\n",
    "            blr.fit(x_slice,y_binary)\n",
    "                # add the trained classifier to the list\n",
    "            print(blr)\n",
    "            self.classifiers_.append(blr)\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        #stochastic takes a single sample \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12936    1\n",
      "47292    2\n",
      "54925    2\n",
      "55453    2\n",
      "82736    2\n",
      "        ..\n",
      "69163    2\n",
      "40068    1\n",
      "283      1\n",
      "22234    1\n",
      "43395    2\n",
      "Name: fit, Length: 1000, dtype: int64\n",
      "[1 2 3]\n",
      "[1 2 3]\n",
      "0\n",
      "1\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-7.79999993]\n",
      " [-0.47191441]\n",
      " [-0.32405496]\n",
      " [-0.19137874]\n",
      " [-2.0909125 ]\n",
      " [-1.92659483]\n",
      " [-3.37396253]\n",
      " [-0.21940475]\n",
      " [-0.44722669]]\n",
      "1\n",
      "2\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 3.26626358]\n",
      " [-0.32558209]\n",
      " [ 0.1460727 ]\n",
      " [-0.01786393]\n",
      " [ 0.11012518]\n",
      " [ 0.18137423]\n",
      " [ 0.03496244]\n",
      " [ 0.01611119]\n",
      " [ 0.07006986]]\n",
      "2\n",
      "3\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-6.80188127]\n",
      " [-0.29377592]\n",
      " [-0.16439056]\n",
      " [-0.1057404 ]\n",
      " [-1.50811676]\n",
      " [-1.3758397 ]\n",
      " [-2.50865353]\n",
      " [-0.06497907]\n",
      " [-0.31569127]]\n"
     ]
    }
   ],
   "source": [
    "#print(y_train)\n",
    "#5194\n",
    "ls = StochasticGradientDescent(eta=0.1,\n",
    "                                           iterations=2500, size=1000,\n",
    "                                           C=.4) # get object(0.1,500)\n",
    "\n",
    "#print( len(y_train))\n",
    "\n",
    "ls.fit(X_train,y_train)\n",
    "#print(ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.6858316221765913\n"
     ]
    }
   ],
   "source": [
    "yhat = ls.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
