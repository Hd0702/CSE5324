{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is the lab...\n",
    "Logistic regression and stuff\n",
    "so yeah... we need paramters for regularization\n",
    "none, L1, L2, this can jsut be a flag with hyperparamters and overloaded functions for the regulairzation...\n",
    "well maybe not overloading but a method that we call that calls other methods called L1 and L2\n",
    "we can compare different regularizations within the same method, that way its legit. \n",
    "for the methods... Were suggetsted to the Stochiastic graident descent. mini-batch gradient descent for our steepest descent and the hessian for the quasi newton thingy. Frick do we gotta write our own hessian? Thatll suck big time\n",
    "\n",
    "Step1.\n",
    "Read data and encode locations as one hot encoded\n",
    "Step2.\n",
    "Split into 80% train and 20 %test after merging.\n",
    "Step3. Write regularization\n",
    "Do these tonight\n",
    "\n",
    "\n",
    "We did both of the regularzations but not together \\for reg we do ridge lasso or elastic net \n",
    "gradient[1:] = regularization + get gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothing-fit-dataset-for-size-recommendation\n",
      "NaNs per column, if more than 20% we will drop otherwise\n",
      "79908 waist\n",
      "0 size\n",
      "68 quality\n",
      "6255 cup size\n",
      "26726 hips\n",
      "6018 bra size\n",
      "70936 bust\n",
      "1107 height\n",
      "35 length\n",
      "54875 shoe size\n",
      "   size  quality  cup size       hips   bra size     height  length  shoe size\n",
      "0     7      5.0       4.0  38.000000  34.000000  77.000000     3.0   8.145818\n",
      "1    13      3.0       2.0  30.000000  36.000000  29.000000     3.0   8.145818\n",
      "2     7      2.0       2.0  40.358501  32.000000  89.000000     4.0   9.000000\n",
      "3    21      5.0       5.0  40.358501  35.972125  67.016136     3.0   8.145818\n",
      "4    18      5.0       2.0  40.358501  36.000000  29.000000     4.0   8.145818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7,  5,  4, ..., 77,  3,  8],\n",
       "       [13,  3,  2, ..., 29,  3,  8],\n",
       "       [ 7,  2,  2, ..., 89,  4,  9],\n",
       "       ...,\n",
       "       [12,  5,  7, ..., 53,  3,  8],\n",
       "       [12,  4,  3, ..., 41,  3,  8],\n",
       "       [ 4,  4,  4, ...,  6,  3,  8]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is the block that simply reads the data\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "def translate_height(height):\n",
    "    height = str(height)\n",
    "    if height == \"nan\":\n",
    "        return None\n",
    "    l = re.findall(r'[0-9]+',height)\n",
    "    if len(l) == 1:\n",
    "        return int(l[0])\n",
    "    else:\n",
    "        return int(l[0]) + int(l[1])*12\n",
    "directory = os.path.relpath(\"clothing-fit-dataset-for-size-recommendation/\")\n",
    "print(directory)\n",
    "df = []\n",
    "y = []\n",
    "df = pd.read_json('clothing-fit-dataset-for-size-recommendation/modcloth_final_data.json', lines=True)\n",
    "di = {\"very short\": 1, \"slightly short\": 2, \"just right\": 3, \"slightly long\": 4, \"very long\": 5}\n",
    "df = df.replace({\"length\": di})\n",
    "di = {\"small\": 1, \"fit\": 2, \"large\": 3}\n",
    "df = df.replace({\"fit\": di})\n",
    "df[\"height\"] = df[\"height\"].map(lambda x: translate_height(x))\n",
    "\n",
    "di = {\"aa\": 0, \"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"dd/e\": 5, \"ddd/f\": 6, \"dddd/g\": 7, \"h\": 8, \"i\": 9, \"j\": 10, \"k\": 11}\n",
    "df = df.replace({\"cup size\": di})\n",
    "#maybe add shoe width back?\n",
    "y = df['fit']\n",
    "df = df.drop(columns=[\"item_id\", \"category\", \"user_name\", \"user_id\", \"shoe width\", \"review_summary\", \"review_text\", \"fit\"])\n",
    "print(\"NaNs per column, if more than 20% we will drop otherwise\")\n",
    "for column in df:\n",
    "    print(df[column].isna().sum(), column)\n",
    "    df[column] = pd.to_numeric(df[column], errors=\"coerce\")\n",
    "df = df.drop(columns=[\"waist\", \"bust\"])\n",
    "df = df.fillna(df.mean())\n",
    "print(df.head())\n",
    "#big issue here is predicting columns with shoes? wtf there are so many NaNs. Not sure what to do about shoe products?\n",
    "x = df.values.astype(int)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        2\n",
       "4        1\n",
       "        ..\n",
       "82785    2\n",
       "82786    1\n",
       "82787    2\n",
       "82788    2\n",
       "82789    2\n",
       "Name: fit, Length: 82790, dtype: int64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is for the renttherunway stuff\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53426    2\n",
       "44377    2\n",
       "69382    2\n",
       "76387    3\n",
       "68028    1\n",
       "        ..\n",
       "24726    2\n",
       "35757    2\n",
       "64967    2\n",
       "15948    2\n",
       "8073     2\n",
       "Name: fit, Length: 16558, dtype: int64"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now split into training and testing data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = .80)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydendonofrio/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/haydendonofrio/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.6906178282401256\n"
     ]
    }
   ],
   "source": [
    "#test logistic regresion to see if my splits are working\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = LogisticRegression(fit_intercept=False)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "yhat = clf.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regurlzarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # this typically gives us the probability of a specific class\n",
    "        # we need a bias term \n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        # this will take the x with the bias term and multiplies it by w. This will give us a scalr quantity\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        # if we actually want to do a prediction we are seeing if it is greater than point 5. vector of ones were it is greater than .5 and 0 when it is below .5\n",
    "        # When one is greater than the other it would be greater than .5\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    #private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    def _get_gradient(self,X,y):\n",
    "        # programming \\sum_i (yi-g(xi))xi\n",
    "        #for each vector x we want to out it throught he sigmoid suntract it from y and add it to the gradient function\n",
    "        gradient = np.zeros(self.w_.shape) # set gradient to zero\n",
    "        for (xi,yi) in zip(X,y):\n",
    "            # the actual update inside of sum\n",
    "            gradi = (yi - self.predict_proba(xi,add_bias=False))*xi \n",
    "            # reshape to be column vector and add to gradient\n",
    "            gradient += gradi.reshape(self.w_.shape) \n",
    "        #return gradient divided by the total number of points\n",
    "        return gradient/float(len(y))\n",
    "       \n",
    "    # public:\n",
    "    def fit(self, X, y):\n",
    "        #this is going to get features and labels. We are going to say x wtih a bias is ther (col of 1s)\n",
    "        #the shape is the # of samples (rows ) by # of features (cols)\n",
    "        #w always starts at 0 usually. \n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        # whats the gradient?? and internall we are updating self.w_ in this\n",
    "        \n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            #put regularization here\n",
    "            regularization = \"L1\"\n",
    "            \n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "class VectorBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    # inherit from our previous class to get same functionality\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # but overwrite the gradient calculation\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        return gradient.reshape(self.w_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, eta, iterations=20):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval) # create a binary problem whenever class is 0 (big class), y binary will be 1\n",
    "            # train the binary classifier for this class\n",
    "            blr = VectorBinaryLogisticRegression(self.eta,self.iters)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X)) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.unique_[np.argmax(self.predict_proba(X),axis=1)] # take argmax along row\n",
    "    \n",
    "#print(X)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yhat = lr.predict(X_test)\n",
    "#print('Accuracy of: ',accuracy_score(y_test,yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedBinaryLogisticRegression(VectorBinaryLogisticRegression):\n",
    "    # extend init functions\n",
    "    def __init__(self, C=0.0,reg=\"BOTH\", **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        self.reg = reg\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "        \n",
    "    # extend previous class to change functionality\n",
    "    def _get_gradient(self,X,y):\n",
    "        # call get gradient from previous class\n",
    "        gradient = super()._get_gradient(X,y)\n",
    "        \n",
    "        # add in regularization (to all except bias term)\n",
    "        #this is L1\n",
    "        div = np.divide(np.absolute(self.w_[1:]).T[0], self.w_[1:,0], out=np.zeros_like(np.absolute(self.w_[1:]).T[0]), where=self.w_[1:,0]!=0)\n",
    "        div = div.T.reshape(self.w_[1:].shape)\n",
    "        if self.reg == \"L1\" or self.reg == \"BOTH\":\n",
    "            gradient[1:] += -1 * div  * self.C\n",
    "        if self.reg == \"BOTH\" or self.reg == \"L2\":\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now redefine the Logistic Regression Function where needed\n",
    "class RegularizedLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, C=0.0,reg=\"BOTH\", **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.C = C\n",
    "        self.reg = reg\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.unique(y) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        \n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = y==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                      C=self.C,\n",
    "                                                     reg=self.reg)\n",
    "            blr.fit(X,y_binary)\n",
    "            # add the trained classifier to the list\n",
    "            print(blr)\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ -0.14718372]\n",
      " [  2.04330978]\n",
      " [-15.60650757]\n",
      " [  1.80187725]\n",
      " [  1.14176384]\n",
      " [  0.9230123 ]\n",
      " [  0.334465  ]\n",
      " [ -7.79787013]\n",
      " [ -0.70236561]]\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n",
      "L2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-246-8c8697a65c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                            \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   reg = \"L2\") # get object(0.1,500)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-245-16226782301d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                                       \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                      reg=self.reg)\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mblr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;31m# add the trained classifier to the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-213-3f835d1ed1e3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;31m#put regularization here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mregularization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"L1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-241-d39e63e2cf12>\u001b[0m in \u001b[0;36m_get_gradient\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# call get gradient from previous class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# add in regularization (to all except bias term)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-214-0a1457e1d49b>\u001b[0m in \u001b[0;36m_get_gradient\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mydiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madd_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get y difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mydiff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make ydiff a column vector and multiply through\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1108\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_values_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;31m# mpl hackaround\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_any_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_values\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             return self._constructor(\n\u001b[0;32m-> 1180\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             ).__finalize__(self)\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mget_slice\u001b[0;34m(self, slobj, axis)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m         return self.__class__(\n\u001b[0;32m-> 1546\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m         )\n\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4291\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4293\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpromote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4294\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4295\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/numeric.py\u001b[0m in \u001b[0;36m_shallow_copy\u001b[0;34m(self, values, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_na\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Ensure we are not returning an Int64Index with float data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shallow_copy_with_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shallow_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_shallow_copy_with_infer\u001b[0;34m(self, values, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, fastpath, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;31m# Constructors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m     def __new__(\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = RegularizedLogisticRegression(eta=0.1,\n",
    "                                           iterations=2500,\n",
    "                                           C=.4, \n",
    "                                  reg = \"L2\") # get object(0.1,500)\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-6c833967e974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy of: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-215-71e934a49f90>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# take argmax along row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#print(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-215-71e934a49f90>\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get probability for each classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make into single matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minibatch s essentially just running regression a smaller batch. This will be much faster with hopefully similar accuracy.\n",
    "from numpy import random\n",
    "class StochasticGradientDescent(RegularizedLogisticRegression):\n",
    "    def __init__(self,size=1000,reg=\"BOTH\", **kwds):\n",
    "        self.reg = reg\n",
    "        self.size = size\n",
    "            #make sure to pass c, eta and iterations\n",
    "        super().__init__(**kwds) # call parent initializer\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        #get a random sample \n",
    "        x_slice = X[self.size:self.size*2,:]\n",
    "        y_slice = y[self.size:self.size*2]\n",
    "        print(y_slice)\n",
    "        self.unique_ = np.unique(y_slice) # get each unique class value\n",
    "        \n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = [] # will fill this array with binary classifiers\n",
    "        #then we will\n",
    "        print(np.unique(y_slice))\n",
    "        #there are nans in here\n",
    "        print(self.unique_)\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            print(i)\n",
    "            print(yval)\n",
    "            y_binary = y_slice==yval # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            blr = RegularizedBinaryLogisticRegression(eta=self.eta,\n",
    "                                                      iterations=self.iters,\n",
    "                                                          C=self.C,\n",
    "                                                     reg=self.reg)\n",
    "            blr.fit(x_slice,y_binary)\n",
    "                # add the trained classifier to the list\n",
    "            print(blr)\n",
    "            self.classifiers_.append(blr)\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        #stochastic takes a single sample \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOTH\n",
      "7538     1\n",
      "42976    1\n",
      "735      3\n",
      "57779    2\n",
      "40611    2\n",
      "        ..\n",
      "42937    3\n",
      "60182    2\n",
      "41920    2\n",
      "57924    2\n",
      "10125    3\n",
      "Name: fit, Length: 1000, dtype: int64\n",
      "[1 2 3]\n",
      "[1 2 3]\n",
      "0\n",
      "1\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-7.81926742]\n",
      " [ 0.16098757]\n",
      " [ 0.08866136]\n",
      " [ 0.04691105]\n",
      " [ 0.25388538]\n",
      " [ 0.20761146]\n",
      " [ 0.15693148]\n",
      " [ 0.05399077]\n",
      " [ 0.10682958]]\n",
      "1\n",
      "2\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[ 1.38736955]\n",
      " [-0.17278521]\n",
      " [ 0.05580851]\n",
      " [ 0.01100682]\n",
      " [ 0.0720296 ]\n",
      " [ 0.01508254]\n",
      " [ 0.51181747]\n",
      " [-0.02417063]\n",
      " [ 0.02030222]]\n",
      "2\n",
      "3\n",
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-5.99134479]\n",
      " [-0.29934354]\n",
      " [-0.12458398]\n",
      " [-0.14387702]\n",
      " [-1.71111999]\n",
      " [-1.5760237 ]\n",
      " [-3.32489552]\n",
      " [-0.11732153]\n",
      " [-0.30813248]]\n"
     ]
    }
   ],
   "source": [
    "#print(y_train)\n",
    "#5194\n",
    "ls = StochasticGradientDescent(eta=0.1,\n",
    "                                           iterations=2500, size=1000,\n",
    "                                           C=.4,\n",
    "                              reg=\"L2\") # get object(0.1,500)\n",
    "\n",
    "#print( len(y_train))\n",
    "\n",
    "ls.fit(X_train,y_train)\n",
    "#print(ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.6175866650561662\n"
     ]
    }
   ],
   "source": [
    "yhat = ls.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
